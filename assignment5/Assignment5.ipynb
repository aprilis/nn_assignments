{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5\n",
    "\n",
    "**Submission deadline: last lab session before or on Tuesday, 09.5.17**\n",
    "\n",
    "**Points: 9 + 10 bonus points**\n",
    "\n",
    "\n",
    "## Downloading this notebook\n",
    "\n",
    "This assignment is an Jupyter notebook. Download it by cloning https://github.com/janchorowski/nn_assignments. Follow the instructions in its README for instructions.\n",
    "\n",
    "For programming exerciese add your solutions to the notebook. For math exercies please provide us with answers on paper or type them in the notebook (it supports Latex-like equations).\n",
    "\n",
    "Please do not hesitate to use GitHubâ€™s pull requests to send us corrections!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vendor:  Continuum Analytics, Inc.\n",
      "Package: mkl\n",
      "Message: trial mode expires in 26 days\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modular network implementation\n",
    "\n",
    "This assignment builds on code from Assignment 4, Problem 7. \n",
    "For your convenience, we have copied the code below. Please copy your solution from the old list, or fill in the blanks below to get a working network.\n",
    "\n",
    "In the following cells, I implement in a modular way a feedforward neural network. Please study the code - many network implementations follow a similar pattern.\n",
    "\n",
    "Please make sure that the network trains to nearly 100% accuracy on Iris.\n",
    "\n",
    "## Task\n",
    "\n",
    "Your job is to implement SGD training on MNIST with the following elements:\n",
    "1. SGD + momentum\n",
    "2. weight decay\n",
    "3. early stopping\n",
    "\n",
    "In overall, you should get below **2% testing errors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# These are taken from https://github.com/mila-udem/blocks\n",
    "# \n",
    "\n",
    "class Constant():\n",
    "    \"\"\"Initialize parameters to a constant.\n",
    "    The constant may be a scalar or a :class:`~numpy.ndarray` of any shape\n",
    "    that is broadcastable with the requested parameter arrays.\n",
    "    Parameters\n",
    "    ----------\n",
    "    constant : :class:`~numpy.ndarray`\n",
    "        The initialization value to use. Must be a scalar or an ndarray (or\n",
    "        compatible object, such as a nested list) that has a shape that is\n",
    "        broadcastable with any shape requested by `initialize`.\n",
    "    \"\"\"\n",
    "    def __init__(self, constant):\n",
    "        self._constant = numpy.asarray(constant)\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        dest = numpy.empty(shape, dtype=np.float32)\n",
    "        dest[...] = self._constant\n",
    "        return dest\n",
    "\n",
    "\n",
    "class IsotropicGaussian():\n",
    "    \"\"\"Initialize parameters from an isotropic Gaussian distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    std : float, optional\n",
    "        The standard deviation of the Gaussian distribution. Defaults to 1.\n",
    "    mean : float, optional\n",
    "        The mean of the Gaussian distribution. Defaults to 0\n",
    "    Notes\n",
    "    -----\n",
    "    Be careful: the standard deviation goes first and the mean goes\n",
    "    second!\n",
    "    \"\"\"\n",
    "    def __init__(self, std=1, mean=0):\n",
    "        self._mean = mean\n",
    "        self._std = std\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        m = rng.normal(self._mean, self._std, size=shape)\n",
    "        return m.astype(np.float32)\n",
    "\n",
    "\n",
    "class Uniform():\n",
    "    \"\"\"Initialize parameters from a uniform distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    mean : float, optional\n",
    "        The mean of the uniform distribution (i.e. the center of mass for\n",
    "        the density function); Defaults to 0.\n",
    "    width : float, optional\n",
    "        One way of specifying the range of the uniform distribution. The\n",
    "        support will be [mean - width/2, mean + width/2]. **Exactly one**\n",
    "        of `width` or `std` must be specified.\n",
    "    std : float, optional\n",
    "        An alternative method of specifying the range of the uniform\n",
    "        distribution. Chooses the width of the uniform such that random\n",
    "        variates will have a desired standard deviation. **Exactly one** of\n",
    "        `width` or `std` must be specified.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=0., width=None, std=None):\n",
    "        if (width is not None) == (std is not None):\n",
    "            raise ValueError(\"must specify width or std, \"\n",
    "                             \"but not both\")\n",
    "        if std is not None:\n",
    "            # Variance of a uniform is 1/12 * width^2\n",
    "            self._width = numpy.sqrt(12) * std\n",
    "        else:\n",
    "            self._width = width\n",
    "        self._mean = mean\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        w = self._width / 2\n",
    "        m = rng.uniform(self._mean - w, self._mean + w, size=shape)\n",
    "        return m.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self, rng=None):\n",
    "        if rng is None:\n",
    "            rng = numpy.random\n",
    "        self.rng = rng\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        return []\n",
    "    \n",
    "    def get_gradients(self, dLdY, fprop_context):\n",
    "        return []\n",
    "    \n",
    "\n",
    "class AffineLayer(Layer):\n",
    "    def __init__(self, num_in, num_out, weight_init=None, bias_init=None, **kwargs):\n",
    "        super(AffineLayer, self).__init__(**kwargs)\n",
    "        if weight_init is None:\n",
    "            #\n",
    "            # TODO propose a default initialization scheme.\n",
    "            # Type a sentence explaining why, and if you use a reference, \n",
    "            # cite it here\n",
    "            #\n",
    "            weight_init = IsotropicGaussian(std=1.0/num_in)\n",
    "        if bias_init is None:\n",
    "            bias_init = Constant(0.0)\n",
    "        \n",
    "        self.W = weight_init.generate(self.rng, (num_out, num_in))\n",
    "        self.b = bias_init.generate(self.rng, (num_out, 1))\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return [self.W, self.b]\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        return ['W','b']\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        #Save X for later reusal\n",
    "        fprop_context = dict(X=X)\n",
    "        Y = np.dot(self.W, X) +  self.b\n",
    "        return Y, fprop_context\n",
    "    \n",
    "    def bprop(self, dLdY, fprop_context):\n",
    "        #\n",
    "        # TODO: fill in gradient computation\n",
    "        #\n",
    "        dLdX = self.W.T.dot(dLdY)\n",
    "        return dLdX\n",
    "    \n",
    "    def get_gradients(self, dLdY, fprop_context):\n",
    "        X = fprop_context['X']\n",
    "        dLdW = np.dot(dLdY, X.T)\n",
    "        dLdb = dLdY.sum(1, keepdims=True)\n",
    "        return [dLdW, dLdb]\n",
    "    \n",
    "class TanhLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(TanhLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        Y = np.tanh(X)\n",
    "        fprop_context = dict(Y=Y)\n",
    "        return Y, fprop_context\n",
    "    \n",
    "    def bprop(self, dLdY, fprop_context):\n",
    "        Y = fprop_context['Y']\n",
    "        #\n",
    "        # Fill in proper gradient computation\n",
    "        #\n",
    "        return dLdY * (1 - Y ** 2)\n",
    "\n",
    "    \n",
    "class ReLULayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ReLULayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        Y = np.maximum(X, 0.0)\n",
    "        fprop_context = dict(Y=Y)\n",
    "        return Y, fprop_context\n",
    "    \n",
    "    def bprop(self, dLdY, fprop_context):\n",
    "        Y = fprop_context['Y']\n",
    "        return dLdY * (Y>0)\n",
    "    \n",
    "class DropoutLayer(Layer):\n",
    "    dropout = True\n",
    "    \n",
    "    def __init__(self, drop_p=0.5, **kwargs):\n",
    "        self.drop_p = drop_p\n",
    "        super(DropoutLayer, self).__init__(**kwargs)\n",
    "        \n",
    "    def fprop(self, X):\n",
    "        if DropoutLayer.dropout:\n",
    "            D = np.random.random(X.shape) > self.drop_p\n",
    "        else:\n",
    "            D = np.ones_like(X) * (1 - self.drop_p)\n",
    "        fprop_context = dict(D=D)\n",
    "        return X * D, fprop_context\n",
    "    \n",
    "    def bprop(self, dLdY, fprop_context):\n",
    "        D = fprop_context['D']\n",
    "        return dLdY * D\n",
    "\n",
    "    \n",
    "class SoftMaxLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SoftMaxLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def compute_probabilities(self, X):\n",
    "        O = X - X.max(axis=0, keepdims=True)\n",
    "        O = np.exp(O)\n",
    "        O /= O.sum(axis=0, keepdims=True)\n",
    "        return O\n",
    "    \n",
    "    def fprop_cost(self, X, Y):\n",
    "        NS = X.shape[1]\n",
    "        O = self.compute_probabilities(X)\n",
    "        Cost = -1.0/NS * np.log(O[Y.ravel(), range(NS)]).sum()\n",
    "        return Cost, O, dict(O=O, X=X, Y=Y)\n",
    "    \n",
    "    def bprop_cost(self, fprop_context):\n",
    "        X = fprop_context['X']\n",
    "        Y = fprop_context['Y']\n",
    "        O = fprop_context['O']\n",
    "        NS = X.shape[1]\n",
    "        dLdX = O.copy()\n",
    "        dLdX[Y, range(NS)] -= 1.0\n",
    "        dLdX /= NS\n",
    "        return dLdX\n",
    "    \n",
    "class FeedForwardNet(object):\n",
    "    def __init__(self, layers=None):\n",
    "        if layers is None:\n",
    "            layers = []\n",
    "        self.layers = layers\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params += layer.parameters\n",
    "        return params\n",
    "    \n",
    "    @parameters.setter\n",
    "    def parameters(self, values):\n",
    "        for ownP, newP in zip(self.parameters, values):\n",
    "            ownP[...] = newP\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        param_names = []\n",
    "        for layer in self.layers:\n",
    "            param_names += layer.parameter_names\n",
    "        return param_names\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        for layer in self.layers[:-1]:\n",
    "            X, fp_context = layer.fprop(X)\n",
    "        return self.layers[-1].compute_probabilities(X)\n",
    "    \n",
    "    def get_cost_and_gradient(self, X, Y):\n",
    "        fp_contexts = []\n",
    "        for layer in self.layers[:-1]:\n",
    "            X, fp_context = layer.fprop(X)\n",
    "            fp_contexts.append(fp_context)\n",
    "        \n",
    "        L, O, fp_context = self.layers[-1].fprop_cost(X, Y)\n",
    "        dLdX = self.layers[-1].bprop_cost(fp_context)\n",
    "        \n",
    "        dLdP = [] #gradient with respect to parameters\n",
    "        for i in xrange(len(self.layers)-1):\n",
    "            layer = self.layers[len(self.layers)-2-i]\n",
    "            fp_context = fp_contexts[len(self.layers)-2-i]\n",
    "            dLdP = layer.get_gradients(dLdX, fp_context) + dLdP\n",
    "            dLdX = layer.bprop(dLdX, fp_context)\n",
    "        return L, O, dLdP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#training algorithms. They change the network!\n",
    "def GD(net, X, Y, alpha=1e-4, max_iters=1000000, tolerance=1e-6):\n",
    "    \"\"\"\n",
    "    Simple batch gradient descent\n",
    "    \"\"\"\n",
    "    old_L = np.inf\n",
    "    for i in xrange(max_iters):\n",
    "        L, O, gradients = net.get_cost_and_gradient(X, Y)\n",
    "        #if old_L < L:\n",
    "        #    print \"Iter: %d, loss increased!!\" % (i,)\n",
    "        #if (old_L - L)<tolerance:\n",
    "        #    print \"Tolerance level reached exiting\"\n",
    "        #    break\n",
    "        if i % 1000 == 0:\n",
    "            err_rate = (O.argmax(0) != Y).mean()\n",
    "            print \"At iteration %d, loss %f, train error rate %f%%\" % (i, L, err_rate*100)\n",
    "        for P,G in zip(net.parameters, gradients):\n",
    "            P -= alpha * G\n",
    "        old_L = L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "IrisX = iris.data.T\n",
    "IrisX = (IrisX - IrisX.mean(axis=1, keepdims=True)) / IrisX.std(axis=1, keepdims=True)\n",
    "IrisY = iris.target.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Here we verify that the network can be trained on Irises.\n",
    "# Most runs should result in 100% accurracy\n",
    "#\n",
    "\n",
    "net = FeedForwardNet([\n",
    "        AffineLayer(4,10),\n",
    "        TanhLayer(),\n",
    "        DropoutLayer(),\n",
    "        AffineLayer(10,3),\n",
    "        SoftMaxLayer()\n",
    "        ])\n",
    "GD(net, IrisX,IrisY, 1e-1, tolerance=1e-7, max_iters=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data from Fuel\n",
    "\n",
    "The following cell prepares the data pipeline in fuel. please see SGD template for usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vendor:  Continuum Analytics, Inc.\n",
      "Package: mkl\n",
      "Message: trial mode expires in 29 days\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a6e98f602120>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m MNIST.default_transformers = (\n\u001b[0;32m     13\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[0mScaleAndShift\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m2.0\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m255.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'which_sources'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'features'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[1;33m(\u001b[0m\u001b[0mCast\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'which_sources'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'features'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'which_sources'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'features'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     (Mapping, [lambda batch: (b.T for b in batch)], {}) )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "from fuel.datasets.mnist import MNIST\n",
    "from fuel.transformers import ScaleAndShift, Cast, Flatten, Mapping\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import SequentialScheme, ShuffledScheme\n",
    "\n",
    "MNIST.default_transformers = (\n",
    "    (ScaleAndShift, [2.0 / 255.0, -1], {'which_sources': 'features'}),\n",
    "    (Cast, [np.float32], {'which_sources': 'features'}), \n",
    "    (Flatten, [], {'which_sources': 'features'}),\n",
    "    (Mapping, [lambda batch: (b.T for b in batch)], {}) )\n",
    "\n",
    "mnist_train = MNIST((\"train\",), subset=slice(None,50000))\n",
    "#this stream will shuffle the MNIST set and return us batches of 100 examples\n",
    "mnist_train_stream = DataStream.default_stream(\n",
    "    mnist_train,\n",
    "    iteration_scheme=ShuffledScheme(mnist_train.num_examples, 100))\n",
    "                                               \n",
    "mnist_validation = MNIST((\"train\",), subset=slice(50000, None))\n",
    "\n",
    "# We will use larger portions for testing and validation\n",
    "# as these dont do a backward pass and reauire less RAM.\n",
    "mnist_validation_stream = DataStream.default_stream(\n",
    "    mnist_validation, iteration_scheme=SequentialScheme(mnist_validation.num_examples, 250))\n",
    "mnist_test = MNIST((\"test\",))\n",
    "mnist_test_stream = DataStream.default_stream(\n",
    "    mnist_test, iteration_scheme=SequentialScheme(mnist_test.num_examples, 250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"The streams return batches containing %s\" % (mnist_train_stream.sources,)\n",
    "\n",
    "print \"Each trainin batch consits of a tuple containing:\"\n",
    "for element in next(mnist_train_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)\n",
    "    \n",
    "print \"Validation/test batches consits of tuples containing:\"\n",
    "for element in next(mnist_test_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 [4p]\n",
    "\n",
    "Implement the following additions to the SGD code below:\n",
    "1. Momentum [2p]\n",
    "2. Learning rate schedule [1p]\n",
    "3. Weight decay [1p]. One way to implement it is to use the functions `net.params` and `net.param_names` to get all parameters whose names are \"W\" and not \"b\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Please note, the code blow is able to train a SoftMax regression model on mnist to poor results (ca 8%test error), \n",
    "# you must improve it\n",
    "#\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "def compute_error_rate(net, stream):\n",
    "    d = DropoutLayer.dropout\n",
    "    DropoutLayer.dropout = False\n",
    "    num_errs = 0.0\n",
    "    num_examples = 0\n",
    "    for X, Y in stream.get_epoch_iterator():\n",
    "        O = net.fprop(X)\n",
    "        num_errs += (O.argmax(0) != Y).sum()\n",
    "        num_examples += X.shape[1]\n",
    "    DropoutLayer.dropout = d\n",
    "    return num_errs/num_examples\n",
    "\n",
    "def SGD(net, train_stream, validation_stream, test_stream, decay_param=0, alpha=-0.2, normlimit=10):\n",
    "    i=0\n",
    "    e=0\n",
    "    \n",
    "    #initialize momentum variables\n",
    "    #\n",
    "    # TODO\n",
    "    #\n",
    "    # Hint: you need one valocity matrix for each parameter\n",
    "    velocities = [np.zeros_like(P) for P in net.parameters]\n",
    "    \n",
    "    best_valid_error_rate = np.inf\n",
    "    best_params = deepcopy(net.parameters)\n",
    "    best_params_epoch = 0\n",
    "    \n",
    "    train_erros = []\n",
    "    train_loss = []\n",
    "    validation_errors = []\n",
    "    \n",
    "    number_of_epochs = 20\n",
    "    patience_expansion = 1.3\n",
    "    \n",
    "    try:\n",
    "        DropoutLayer.dropout = True\n",
    "        while e<number_of_epochs: #This loop goes over epochs\n",
    "            e += 1\n",
    "            #First train on all data from this batch\n",
    "            max_norm = 0\n",
    "            for X,Y in train_stream.get_epoch_iterator(): \n",
    "                i += 1\n",
    "                L, O, gradients = net.get_cost_and_gradient(X, Y)\n",
    "                err_rate = (O.argmax(0) != Y).mean()\n",
    "                train_loss.append((i,L))\n",
    "                train_erros.append((i,err_rate))\n",
    "                if i % 100 == 0:\n",
    "                    print \"At minibatch %d, batch loss %f, batch error rate %f%%\" % (i, L, err_rate*100)\n",
    "                    print 'max norm =', max_norm\n",
    "                    print 'velocity =', sum(map(np.linalg.norm, velocities))\n",
    "                if i % 5000 == 0: alpha *= 0.7\n",
    "                for P, V, G, N in zip(net.parameters, velocities, gradients, net.parameter_names):\n",
    "                    if N=='W':\n",
    "                        #\n",
    "                        # TODO: implement the weight decay addition to gradient\n",
    "                        #\n",
    "                        G += P * decay_param\n",
    "                    \n",
    "                    #\n",
    "                    # TODO: set a learning rate\n",
    "                    #\n",
    "                    # Hint, use the iteration counter i\n",
    "                    \n",
    "                    #\n",
    "                    # TODO: set the momentum constant \n",
    "                    # \n",
    "                    \n",
    "                    epsilon = 0.8\n",
    "                    \n",
    "                    #\n",
    "                    # TODO: implement velocity update in momentum\n",
    "                    #\n",
    "                    \n",
    "                    V[...] = V * epsilon + G * alpha\n",
    "                    \n",
    "                    #\n",
    "                    # TODO: set a more sensible learning rule here,\n",
    "                    # using your learning rate schedule and momentum\n",
    "                    #\n",
    "                    #!!!!! Need to modify the actual parameter here! \n",
    "                    P += V\n",
    "                    \n",
    "                    norms = np.linalg.norm(P, axis=1)\n",
    "                    max_norm = max(max_norm, norms.max())\n",
    "                    factor = np.minimum(normlimit / norms, 1)\n",
    "                    P *= factor[:,None]\n",
    "                    \n",
    "            # After an epoch compute validation error\n",
    "            d = DropoutLayer.dropout\n",
    "            DropoutLayer.dropout = False\n",
    "            val_error_rate = compute_error_rate(net, validation_stream)\n",
    "            DropoutLayer.dropout = d\n",
    "            if val_error_rate < best_valid_error_rate:\n",
    "                number_of_epochs = np.maximum(number_of_epochs, e * patience_expansion+1)\n",
    "                best_valid_error_rate = val_error_rate\n",
    "                best_params = deepcopy(net.parameters)\n",
    "                best_params_epoch = e\n",
    "                validation_errors.append((i,val_error_rate))\n",
    "            print \"After epoch %d: valid_err_rate: %f%% currently going ot do %d epochs\" %(\n",
    "                e, val_error_rate, number_of_epochs)\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print \"Setting network parameters from after epoch %d\" %(best_params_epoch)\n",
    "        net.parameters = best_params\n",
    "        \n",
    "        subplot(2,1,1)\n",
    "        train_loss = np.array(train_loss)\n",
    "        semilogy(train_loss[:,0], train_loss[:,1], label='batch train loss')\n",
    "        legend()\n",
    "        \n",
    "        subplot(2,1,2)\n",
    "        train_erros = np.array(train_erros)\n",
    "        plot(train_erros[:,0], train_erros[:,1], label='batch train error rate')\n",
    "        validation_errors = np.array(validation_errors)\n",
    "        plot(validation_errors[:,0], validation_errors[:,1], label='validation error rate', color='r')\n",
    "        ylim(0,0.2)\n",
    "        legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 [5p]\n",
    "\n",
    "Tune the following network to reach below 1.9% error rate on\n",
    "the validation set. This should result in a test error below 2%. To\n",
    "tune the network you will need to:\n",
    "1. choose the number of layers (more than 1, less than 5),\n",
    "2. choose the number of neurons in each layer (more than 100,\n",
    "    less than 5000),\n",
    "3. pick proper weight initialization,\n",
    "4. pick proper learning rate schedule (need to decay over time,\n",
    "    good range to check on MNIST is about 1e-2 ... 1e-1 at the beginning and\n",
    "    half of that after 10000 batches),\n",
    "5. pick a momentum constant (probably a constant one will be OK).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compute_error_rate(net, mnist_test_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At minibatch 100, batch loss 2.164436, batch error rate 76.000000%\n",
      "max norm = 0.663413\n",
      "velocity = 0.83337\n",
      "At minibatch 200, batch loss 0.328114, batch error rate 12.000000%\n",
      "max norm = 1.14388\n",
      "velocity = 0.313121\n",
      "At minibatch 300, batch loss 0.463268, batch error rate 14.000000%\n",
      "max norm = 1.15496\n",
      "velocity = 0.373011\n",
      "At minibatch 400, batch loss 0.228991, batch error rate 8.000000%\n",
      "max norm = 1.18062\n",
      "velocity = 0.264304\n",
      "At minibatch 500, batch loss 0.109574, batch error rate 3.000000%\n",
      "max norm = 1.25369\n",
      "velocity = 0.258753\n",
      "After epoch 1: valid_err_rate: 0.079200% currently going ot do 20 epochs\n",
      "At minibatch 600, batch loss 0.158996, batch error rate 4.000000%\n",
      "max norm = 1.29624\n",
      "velocity = 0.261607\n",
      "At minibatch 700, batch loss 0.118604, batch error rate 2.000000%\n",
      "max norm = 1.3335\n",
      "velocity = 0.270976\n",
      "At minibatch 800, batch loss 0.259716, batch error rate 8.000000%\n",
      "max norm = 1.34649\n",
      "velocity = 0.29671\n",
      "At minibatch 900, batch loss 0.158484, batch error rate 6.000000%\n",
      "max norm = 1.40289\n",
      "velocity = 0.243417\n",
      "At minibatch 1000, batch loss 0.107694, batch error rate 4.000000%\n",
      "max norm = 1.48455\n",
      "velocity = 0.26738\n",
      "After epoch 2: valid_err_rate: 0.050200% currently going ot do 20 epochs\n",
      "At minibatch 1100, batch loss 0.202091, batch error rate 7.000000%\n",
      "max norm = 1.5221\n",
      "velocity = 0.218635\n",
      "At minibatch 1200, batch loss 0.155792, batch error rate 5.000000%\n",
      "max norm = 1.5683\n",
      "velocity = 0.262527\n",
      "At minibatch 1300, batch loss 0.180227, batch error rate 6.000000%\n",
      "max norm = 1.59381\n",
      "velocity = 0.179991\n",
      "At minibatch 1400, batch loss 0.124037, batch error rate 3.000000%\n",
      "max norm = 1.6584\n",
      "velocity = 0.235289\n",
      "At minibatch 1500, batch loss 0.266067, batch error rate 10.000000%\n",
      "max norm = 1.69913\n",
      "velocity = 0.230727\n",
      "After epoch 3: valid_err_rate: 0.040400% currently going ot do 20 epochs\n",
      "At minibatch 1600, batch loss 0.094091, batch error rate 4.000000%\n",
      "max norm = 1.69658\n",
      "velocity = 0.223604\n",
      "At minibatch 1700, batch loss 0.203885, batch error rate 7.000000%\n",
      "max norm = 1.69658\n",
      "velocity = 0.263625\n",
      "At minibatch 1800, batch loss 0.126593, batch error rate 2.000000%\n",
      "max norm = 1.69658\n",
      "velocity = 0.208035\n",
      "At minibatch 1900, batch loss 0.094076, batch error rate 3.000000%\n",
      "max norm = 1.70788\n",
      "velocity = 0.203559\n",
      "At minibatch 2000, batch loss 0.144345, batch error rate 6.000000%\n",
      "max norm = 1.70917\n",
      "velocity = 0.218218\n",
      "After epoch 4: valid_err_rate: 0.032800% currently going ot do 20 epochs\n",
      "At minibatch 2100, batch loss 0.107058, batch error rate 5.000000%\n",
      "max norm = 1.70328\n",
      "velocity = 0.152744\n",
      "At minibatch 2200, batch loss 0.165724, batch error rate 6.000000%\n",
      "max norm = 1.74065\n",
      "velocity = 0.235202\n",
      "At minibatch 2300, batch loss 0.080251, batch error rate 3.000000%\n",
      "max norm = 1.75754\n",
      "velocity = 0.322139\n",
      "At minibatch 2400, batch loss 0.095114, batch error rate 3.000000%\n",
      "max norm = 1.75754\n",
      "velocity = 0.19486\n",
      "At minibatch 2500, batch loss 0.108313, batch error rate 4.000000%\n",
      "max norm = 1.77411\n",
      "velocity = 0.23689\n",
      "After epoch 5: valid_err_rate: 0.033000% currently going ot do 20 epochs\n",
      "At minibatch 2600, batch loss 0.238493, batch error rate 6.000000%\n",
      "max norm = 1.80894\n",
      "velocity = 0.200779\n",
      "At minibatch 2700, batch loss 0.122221, batch error rate 3.000000%\n",
      "max norm = 1.82531\n",
      "velocity = 0.255841\n",
      "At minibatch 2800, batch loss 0.132335, batch error rate 4.000000%\n",
      "max norm = 1.82531\n",
      "velocity = 0.301141\n",
      "At minibatch 2900, batch loss 0.109002, batch error rate 3.000000%\n",
      "max norm = 1.82531\n",
      "velocity = 0.272596\n",
      "At minibatch 3000, batch loss 0.142045, batch error rate 6.000000%\n",
      "max norm = 1.82531\n",
      "velocity = 0.218858\n",
      "After epoch 6: valid_err_rate: 0.047500% currently going ot do 20 epochs\n",
      "At minibatch 3100, batch loss 0.079838, batch error rate 3.000000%\n",
      "max norm = 1.78358\n",
      "velocity = 0.144916\n",
      "At minibatch 3200, batch loss 0.039956, batch error rate 1.000000%\n",
      "max norm = 1.78358\n",
      "velocity = 0.226773\n",
      "At minibatch 3300, batch loss 0.054122, batch error rate 2.000000%\n",
      "max norm = 1.81096\n",
      "velocity = 0.241869\n",
      "At minibatch 3400, batch loss 0.057109, batch error rate 1.000000%\n",
      "max norm = 1.81303\n",
      "velocity = 0.223782\n",
      "At minibatch 3500, batch loss 0.068901, batch error rate 2.000000%\n",
      "max norm = 1.84695\n",
      "velocity = 0.349381\n",
      "After epoch 7: valid_err_rate: 0.046500% currently going ot do 20 epochs\n",
      "At minibatch 3600, batch loss 0.029700, batch error rate 0.000000%\n",
      "max norm = 1.84139\n",
      "velocity = 0.176859\n",
      "At minibatch 3700, batch loss 0.096248, batch error rate 3.000000%\n",
      "max norm = 1.84139\n",
      "velocity = 0.281758\n",
      "At minibatch 3800, batch loss 0.061071, batch error rate 1.000000%\n",
      "max norm = 1.84139\n",
      "velocity = 0.20893\n",
      "At minibatch 3900, batch loss 0.124760, batch error rate 3.000000%\n",
      "max norm = 1.84139\n",
      "velocity = 0.190695\n",
      "At minibatch 4000, batch loss 0.182004, batch error rate 5.000000%\n",
      "max norm = 1.84139\n",
      "velocity = 0.193615\n",
      "After epoch 8: valid_err_rate: 0.035200% currently going ot do 20 epochs\n",
      "At minibatch 4100, batch loss 0.106911, batch error rate 2.000000%\n",
      "max norm = 1.79222\n",
      "velocity = 0.168348\n",
      "At minibatch 4200, batch loss 0.151385, batch error rate 6.000000%\n",
      "max norm = 1.79222\n",
      "velocity = 0.207674\n",
      "At minibatch 4300, batch loss 0.119578, batch error rate 5.000000%\n",
      "max norm = 1.79222\n",
      "velocity = 0.258814\n",
      "At minibatch 4400, batch loss 0.037947, batch error rate 1.000000%\n",
      "max norm = 1.79222\n",
      "velocity = 0.22344\n",
      "At minibatch 4500, batch loss 0.107605, batch error rate 3.000000%\n",
      "max norm = 1.79222\n",
      "velocity = 0.234208\n",
      "After epoch 9: valid_err_rate: 0.034700% currently going ot do 20 epochs\n",
      "At minibatch 4600, batch loss 0.059281, batch error rate 1.000000%\n",
      "max norm = 1.79585\n",
      "velocity = 0.251056\n",
      "At minibatch 4700, batch loss 0.086366, batch error rate 2.000000%\n",
      "max norm = 1.79585\n",
      "velocity = 0.234847\n",
      "At minibatch 4800, batch loss 0.087858, batch error rate 2.000000%\n",
      "max norm = 1.80752\n",
      "velocity = 0.202981\n",
      "At minibatch 4900, batch loss 0.074916, batch error rate 2.000000%\n",
      "max norm = 1.80752\n",
      "velocity = 0.173196\n",
      "At minibatch 5000, batch loss 0.076610, batch error rate 2.000000%\n",
      "max norm = 1.80752\n",
      "velocity = 0.271304\n",
      "After epoch 10: valid_err_rate: 0.036500% currently going ot do 20 epochs\n",
      "At minibatch 5100, batch loss 0.041594, batch error rate 0.000000%\n",
      "max norm = 1.75901\n",
      "velocity = 0.112955\n",
      "At minibatch 5200, batch loss 0.103872, batch error rate 4.000000%\n",
      "max norm = 1.75901\n",
      "velocity = 0.128573\n",
      "At minibatch 5300, batch loss 0.068153, batch error rate 5.000000%\n",
      "max norm = 1.75901\n",
      "velocity = 0.106692\n",
      "At minibatch 5400, batch loss 0.148000, batch error rate 5.000000%\n",
      "max norm = 1.75901\n",
      "velocity = 0.131292\n",
      "At minibatch 5500, batch loss 0.049616, batch error rate 1.000000%\n",
      "max norm = 1.75901\n",
      "velocity = 0.142353\n",
      "After epoch 11: valid_err_rate: 0.029000% currently going ot do 20 epochs\n",
      "At minibatch 5600, batch loss 0.088769, batch error rate 3.000000%\n",
      "max norm = 1.64557\n",
      "velocity = 0.110551\n",
      "At minibatch 5700, batch loss 0.067907, batch error rate 3.000000%\n",
      "max norm = 1.64557\n",
      "velocity = 0.174978\n",
      "At minibatch 5800, batch loss 0.065546, batch error rate 2.000000%\n",
      "max norm = 1.64557\n",
      "velocity = 0.120737\n",
      "At minibatch 5900, batch loss 0.031397, batch error rate 1.000000%\n",
      "max norm = 1.65064\n",
      "velocity = 0.118139\n",
      "At minibatch 6000, batch loss 0.065812, batch error rate 2.000000%\n",
      "max norm = 1.65064\n",
      "velocity = 0.114817\n",
      "After epoch 12: valid_err_rate: 0.030600% currently going ot do 20 epochs\n",
      "At minibatch 6100, batch loss 0.061625, batch error rate 1.000000%\n",
      "max norm = 1.66045\n",
      "velocity = 0.106903\n",
      "At minibatch 6200, batch loss 0.023846, batch error rate 1.000000%\n",
      "max norm = 1.69177\n",
      "velocity = 0.221999\n",
      "At minibatch 6300, batch loss 0.046316, batch error rate 2.000000%\n",
      "max norm = 1.69322\n",
      "velocity = 0.131031\n",
      "At minibatch 6400, batch loss 0.065718, batch error rate 2.000000%\n",
      "max norm = 1.69322\n",
      "velocity = 0.144037\n",
      "At minibatch 6500, batch loss 0.061535, batch error rate 3.000000%\n",
      "max norm = 1.69322\n",
      "velocity = 0.132633\n",
      "After epoch 13: valid_err_rate: 0.027600% currently going ot do 20 epochs\n",
      "At minibatch 6600, batch loss 0.060194, batch error rate 3.000000%\n",
      "max norm = 1.67552\n",
      "velocity = 0.112589\n",
      "At minibatch 6700, batch loss 0.064700, batch error rate 3.000000%\n",
      "max norm = 1.67779\n",
      "velocity = 0.153535\n",
      "At minibatch 6800, batch loss 0.047250, batch error rate 1.000000%\n",
      "max norm = 1.68045\n",
      "velocity = 0.110838\n",
      "At minibatch 6900, batch loss 0.093029, batch error rate 3.000000%\n",
      "max norm = 1.70029\n",
      "velocity = 0.128756\n",
      "At minibatch 7000, batch loss 0.034409, batch error rate 2.000000%\n",
      "max norm = 1.70029\n",
      "velocity = 0.129562\n",
      "After epoch 14: valid_err_rate: 0.032100% currently going ot do 20 epochs\n",
      "At minibatch 7100, batch loss 0.033006, batch error rate 0.000000%\n",
      "max norm = 1.67938\n",
      "velocity = 0.133037\n",
      "At minibatch 7200, batch loss 0.089408, batch error rate 2.000000%\n",
      "max norm = 1.70147\n",
      "velocity = 0.126935\n",
      "At minibatch 7300, batch loss 0.051698, batch error rate 1.000000%\n",
      "max norm = 1.71507\n",
      "velocity = 0.103321\n",
      "At minibatch 7400, batch loss 0.058271, batch error rate 3.000000%\n",
      "max norm = 1.71507\n",
      "velocity = 0.186807\n",
      "At minibatch 7500, batch loss 0.038114, batch error rate 2.000000%\n",
      "max norm = 1.71507\n",
      "velocity = 0.200858\n",
      "After epoch 15: valid_err_rate: 0.024800% currently going ot do 20 epochs\n",
      "At minibatch 7600, batch loss 0.048520, batch error rate 1.000000%\n",
      "max norm = 1.70092\n",
      "velocity = 0.120548\n",
      "At minibatch 7700, batch loss 0.048737, batch error rate 2.000000%\n",
      "max norm = 1.70092\n",
      "velocity = 0.100229\n",
      "At minibatch 7800, batch loss 0.096605, batch error rate 1.000000%\n",
      "max norm = 1.72953\n",
      "velocity = 0.123606\n",
      "At minibatch 7900, batch loss 0.103679, batch error rate 2.000000%\n",
      "max norm = 1.72953\n",
      "velocity = 0.169971\n",
      "At minibatch 8000, batch loss 0.113103, batch error rate 6.000000%\n",
      "max norm = 1.72953\n",
      "velocity = 0.142831\n",
      "After epoch 16: valid_err_rate: 0.026300% currently going ot do 20 epochs\n",
      "At minibatch 8100, batch loss 0.055178, batch error rate 2.000000%\n",
      "max norm = 1.68699\n",
      "velocity = 0.107076\n",
      "At minibatch 8200, batch loss 0.046574, batch error rate 1.000000%\n",
      "max norm = 1.70116\n",
      "velocity = 0.185873\n",
      "At minibatch 8300, batch loss 0.135135, batch error rate 6.000000%\n",
      "max norm = 1.71536\n",
      "velocity = 0.11293\n",
      "At minibatch 8400, batch loss 0.046461, batch error rate 1.000000%\n",
      "max norm = 1.71536\n",
      "velocity = 0.165367\n",
      "At minibatch 8500, batch loss 0.086168, batch error rate 3.000000%\n",
      "max norm = 1.71536\n",
      "velocity = 0.104774\n",
      "After epoch 17: valid_err_rate: 0.029200% currently going ot do 20 epochs\n",
      "At minibatch 8600, batch loss 0.030663, batch error rate 0.000000%\n",
      "max norm = 1.72018\n",
      "velocity = 0.124895\n",
      "At minibatch 8700, batch loss 0.038148, batch error rate 1.000000%\n",
      "max norm = 1.72404\n",
      "velocity = 0.208219\n",
      "At minibatch 8800, batch loss 0.053850, batch error rate 3.000000%\n",
      "max norm = 1.7359\n",
      "velocity = 0.134487\n",
      "At minibatch 8900, batch loss 0.060252, batch error rate 2.000000%\n",
      "max norm = 1.7494\n",
      "velocity = 0.109501\n",
      "At minibatch 9000, batch loss 0.008640, batch error rate 0.000000%\n",
      "max norm = 1.7494\n",
      "velocity = 0.130812\n",
      "After epoch 18: valid_err_rate: 0.028200% currently going ot do 20 epochs\n",
      "At minibatch 9100, batch loss 0.029538, batch error rate 1.000000%\n",
      "max norm = 1.74924\n",
      "velocity = 0.0887025\n",
      "At minibatch 9200, batch loss 0.042671, batch error rate 1.000000%\n",
      "max norm = 1.76342\n",
      "velocity = 0.145598\n",
      "At minibatch 9300, batch loss 0.031796, batch error rate 1.000000%\n",
      "max norm = 1.76524\n",
      "velocity = 0.108199\n",
      "At minibatch 9400, batch loss 0.124398, batch error rate 5.000000%\n",
      "max norm = 1.76524\n",
      "velocity = 0.148116\n",
      "At minibatch 9500, batch loss 0.163847, batch error rate 8.000000%\n",
      "max norm = 1.76524\n",
      "velocity = 0.139091\n",
      "After epoch 19: valid_err_rate: 0.029900% currently going ot do 20 epochs\n",
      "At minibatch 9600, batch loss 0.072070, batch error rate 3.000000%\n",
      "max norm = 1.72516\n",
      "velocity = 0.116142\n",
      "At minibatch 9700, batch loss 0.028874, batch error rate 1.000000%\n",
      "max norm = 1.75177\n",
      "velocity = 0.138111\n",
      "At minibatch 9800, batch loss 0.115773, batch error rate 3.000000%\n",
      "max norm = 1.75177\n",
      "velocity = 0.126829\n",
      "At minibatch 9900, batch loss 0.048615, batch error rate 1.000000%\n",
      "max norm = 1.75177\n",
      "velocity = 0.160706\n",
      "At minibatch 10000, batch loss 0.017843, batch error rate 0.000000%\n",
      "max norm = 1.75177\n",
      "velocity = 0.141114\n",
      "After epoch 20: valid_err_rate: 0.024700% currently going ot do 27 epochs\n",
      "At minibatch 10100, batch loss 0.030786, batch error rate 1.000000%\n",
      "max norm = 1.70928\n",
      "velocity = 0.070507\n",
      "At minibatch 10200, batch loss 0.023661, batch error rate 1.000000%\n",
      "max norm = 1.71038\n",
      "velocity = 0.0785988\n",
      "At minibatch 10300, batch loss 0.029968, batch error rate 1.000000%\n",
      "max norm = 1.73206\n",
      "velocity = 0.0538053\n",
      "At minibatch 10400, batch loss 0.049650, batch error rate 1.000000%\n",
      "max norm = 1.74416\n",
      "velocity = 0.0756548\n",
      "At minibatch 10500, batch loss 0.067049, batch error rate 3.000000%\n",
      "max norm = 1.7496\n",
      "velocity = 0.111317\n",
      "After epoch 21: valid_err_rate: 0.023300% currently going ot do 28 epochs\n",
      "At minibatch 10600, batch loss 0.072965, batch error rate 1.000000%\n",
      "max norm = 1.76145\n",
      "velocity = 0.0837246\n",
      "At minibatch 10700, batch loss 0.018985, batch error rate 0.000000%\n",
      "max norm = 1.79114\n",
      "velocity = 0.0540781\n",
      "At minibatch 10800, batch loss 0.059944, batch error rate 3.000000%\n",
      "max norm = 1.79449\n",
      "velocity = 0.0740146\n",
      "At minibatch 10900, batch loss 0.021249, batch error rate 0.000000%\n",
      "max norm = 1.79449\n",
      "velocity = 0.0756796\n",
      "At minibatch 11000, batch loss 0.043265, batch error rate 1.000000%\n",
      "max norm = 1.79449\n",
      "velocity = 0.110079\n",
      "After epoch 22: valid_err_rate: 0.025300% currently going ot do 28 epochs\n",
      "At minibatch 11100, batch loss 0.014438, batch error rate 0.000000%\n",
      "max norm = 1.79216\n",
      "velocity = 0.0676883\n",
      "At minibatch 11200, batch loss 0.067159, batch error rate 3.000000%\n",
      "max norm = 1.80037\n",
      "velocity = 0.0799411\n",
      "At minibatch 11300, batch loss 0.049350, batch error rate 2.000000%\n",
      "max norm = 1.80754\n",
      "velocity = 0.0881088\n",
      "At minibatch 11400, batch loss 0.126490, batch error rate 2.000000%\n",
      "max norm = 1.81703\n",
      "velocity = 0.0903438\n",
      "At minibatch 11500, batch loss 0.018692, batch error rate 0.000000%\n",
      "max norm = 1.82114\n",
      "velocity = 0.0602278\n",
      "After epoch 23: valid_err_rate: 0.024400% currently going ot do 28 epochs\n",
      "At minibatch 11600, batch loss 0.077504, batch error rate 1.000000%\n",
      "max norm = 1.83984\n",
      "velocity = 0.0655125\n",
      "At minibatch 11700, batch loss 0.067574, batch error rate 2.000000%\n",
      "max norm = 1.83984\n",
      "velocity = 0.08115\n",
      "At minibatch 11800, batch loss 0.033547, batch error rate 0.000000%\n",
      "max norm = 1.83984\n",
      "velocity = 0.097922\n",
      "At minibatch 11900, batch loss 0.031808, batch error rate 1.000000%\n",
      "max norm = 1.84946\n",
      "velocity = 0.0854497\n",
      "At minibatch 12000, batch loss 0.024085, batch error rate 0.000000%\n",
      "max norm = 1.85057\n",
      "velocity = 0.0634071\n",
      "After epoch 24: valid_err_rate: 0.022000% currently going ot do 32 epochs\n",
      "At minibatch 12100, batch loss 0.022527, batch error rate 0.000000%\n",
      "max norm = 1.83883\n",
      "velocity = 0.0957424\n",
      "At minibatch 12200, batch loss 0.009006, batch error rate 0.000000%\n",
      "max norm = 1.83883\n",
      "velocity = 0.0595396\n",
      "At minibatch 12300, batch loss 0.039926, batch error rate 1.000000%\n",
      "max norm = 1.84755\n",
      "velocity = 0.103882\n",
      "At minibatch 12400, batch loss 0.070301, batch error rate 4.000000%\n",
      "max norm = 1.84755\n",
      "velocity = 0.104806\n",
      "At minibatch 12500, batch loss 0.058039, batch error rate 2.000000%\n",
      "max norm = 1.85071\n",
      "velocity = 0.0993112\n",
      "After epoch 25: valid_err_rate: 0.023200% currently going ot do 32 epochs\n",
      "At minibatch 12600, batch loss 0.012712, batch error rate 0.000000%\n",
      "max norm = 1.86081\n",
      "velocity = 0.0872082\n",
      "At minibatch 12700, batch loss 0.046539, batch error rate 1.000000%\n",
      "max norm = 1.87073\n",
      "velocity = 0.0761692\n",
      "At minibatch 12800, batch loss 0.061618, batch error rate 1.000000%\n",
      "max norm = 1.87552\n",
      "velocity = 0.0785695\n",
      "At minibatch 12900, batch loss 0.029301, batch error rate 1.000000%\n",
      "max norm = 1.87552\n",
      "velocity = 0.0860597\n",
      "At minibatch 13000, batch loss 0.098740, batch error rate 4.000000%\n",
      "max norm = 1.87552\n",
      "velocity = 0.0980259\n",
      "After epoch 26: valid_err_rate: 0.025400% currently going ot do 32 epochs\n",
      "At minibatch 13100, batch loss 0.033864, batch error rate 0.000000%\n",
      "max norm = 1.86812\n",
      "velocity = 0.13171\n",
      "At minibatch 13200, batch loss 0.060611, batch error rate 3.000000%\n",
      "max norm = 1.87521\n",
      "velocity = 0.116173\n",
      "At minibatch 13300, batch loss 0.049945, batch error rate 2.000000%\n",
      "max norm = 1.87521\n",
      "velocity = 0.0789012\n",
      "At minibatch 13400, batch loss 0.037711, batch error rate 2.000000%\n",
      "max norm = 1.87565\n",
      "velocity = 0.106203\n",
      "At minibatch 13500, batch loss 0.067869, batch error rate 1.000000%\n",
      "max norm = 1.87565\n",
      "velocity = 0.0896732\n",
      "After epoch 27: valid_err_rate: 0.035500% currently going ot do 32 epochs\n",
      "At minibatch 13600, batch loss 0.062568, batch error rate 2.000000%\n",
      "max norm = 1.88241\n",
      "velocity = 0.0864432\n",
      "At minibatch 13700, batch loss 0.057742, batch error rate 2.000000%\n",
      "max norm = 1.88241\n",
      "velocity = 0.076413\n",
      "At minibatch 13800, batch loss 0.027334, batch error rate 1.000000%\n",
      "max norm = 1.89357\n",
      "velocity = 0.161197\n",
      "At minibatch 13900, batch loss 0.050030, batch error rate 1.000000%\n",
      "max norm = 1.89357\n",
      "velocity = 0.093503\n",
      "At minibatch 14000, batch loss 0.047240, batch error rate 2.000000%\n",
      "max norm = 1.89357\n",
      "velocity = 0.0902264\n",
      "After epoch 28: valid_err_rate: 0.022100% currently going ot do 32 epochs\n",
      "At minibatch 14100, batch loss 0.019770, batch error rate 0.000000%\n",
      "max norm = 1.86609\n",
      "velocity = 0.0968444\n",
      "At minibatch 14200, batch loss 0.020122, batch error rate 0.000000%\n",
      "max norm = 1.86609\n",
      "velocity = 0.0742141\n",
      "At minibatch 14300, batch loss 0.051702, batch error rate 3.000000%\n",
      "max norm = 1.86609\n",
      "velocity = 0.0904634\n",
      "At minibatch 14400, batch loss 0.065781, batch error rate 2.000000%\n",
      "max norm = 1.86609\n",
      "velocity = 0.0987823\n",
      "At minibatch 14500, batch loss 0.046249, batch error rate 2.000000%\n",
      "max norm = 1.8739\n",
      "velocity = 0.111343\n",
      "After epoch 29: valid_err_rate: 0.028400% currently going ot do 32 epochs\n",
      "At minibatch 14600, batch loss 0.025656, batch error rate 0.000000%\n",
      "max norm = 1.87171\n",
      "velocity = 0.118315\n",
      "At minibatch 14700, batch loss 0.026431, batch error rate 0.000000%\n",
      "max norm = 1.88991\n",
      "velocity = 0.117576\n",
      "At minibatch 14800, batch loss 0.053634, batch error rate 2.000000%\n",
      "max norm = 1.88991\n",
      "velocity = 0.0594638\n",
      "At minibatch 14900, batch loss 0.035381, batch error rate 1.000000%\n",
      "max norm = 1.88991\n",
      "velocity = 0.0842459\n",
      "At minibatch 15000, batch loss 0.055253, batch error rate 2.000000%\n",
      "max norm = 1.89123\n",
      "velocity = 0.073666\n",
      "After epoch 30: valid_err_rate: 0.022300% currently going ot do 32 epochs\n",
      "At minibatch 15100, batch loss 0.010203, batch error rate 0.000000%\n",
      "max norm = 1.89068\n",
      "velocity = 0.0850679\n",
      "At minibatch 15200, batch loss 0.061694, batch error rate 1.000000%\n",
      "max norm = 1.89068\n",
      "velocity = 0.0296818\n",
      "At minibatch 15300, batch loss 0.015453, batch error rate 0.000000%\n",
      "max norm = 1.89068\n",
      "velocity = 0.0661659\n",
      "At minibatch 15400, batch loss 0.041719, batch error rate 1.000000%\n",
      "max norm = 1.89068\n",
      "velocity = 0.0456723\n",
      "At minibatch 15500, batch loss 0.011953, batch error rate 0.000000%\n",
      "max norm = 1.89068\n",
      "velocity = 0.075818\n",
      "After epoch 31: valid_err_rate: 0.020400% currently going ot do 41 epochs\n",
      "At minibatch 15600, batch loss 0.020279, batch error rate 0.000000%\n",
      "max norm = 1.88697\n",
      "velocity = 0.0505277\n",
      "At minibatch 15700, batch loss 0.025162, batch error rate 0.000000%\n",
      "max norm = 1.89108\n",
      "velocity = 0.0356992\n",
      "At minibatch 15800, batch loss 0.024631, batch error rate 1.000000%\n",
      "max norm = 1.90229\n",
      "velocity = 0.0577096\n",
      "At minibatch 15900, batch loss 0.011537, batch error rate 0.000000%\n",
      "max norm = 1.90779\n",
      "velocity = 0.0273211\n",
      "At minibatch 16000, batch loss 0.015708, batch error rate 0.000000%\n",
      "max norm = 1.90779\n",
      "velocity = 0.0398398\n",
      "After epoch 32: valid_err_rate: 0.019600% currently going ot do 42 epochs\n",
      "At minibatch 16100, batch loss 0.023299, batch error rate 1.000000%\n",
      "max norm = 1.91017\n",
      "velocity = 0.0452692\n",
      "At minibatch 16200, batch loss 0.018225, batch error rate 0.000000%\n",
      "max norm = 1.91017\n",
      "velocity = 0.0374718\n",
      "At minibatch 16300, batch loss 0.046675, batch error rate 1.000000%\n",
      "max norm = 1.91625\n",
      "velocity = 0.0475246\n",
      "At minibatch 16400, batch loss 0.025326, batch error rate 2.000000%\n",
      "max norm = 1.92017\n",
      "velocity = 0.0596714\n",
      "At minibatch 16500, batch loss 0.019317, batch error rate 0.000000%\n",
      "max norm = 1.92935\n",
      "velocity = 0.0338123\n",
      "After epoch 33: valid_err_rate: 0.019400% currently going ot do 43 epochs\n",
      "At minibatch 16600, batch loss 0.046031, batch error rate 1.000000%\n",
      "max norm = 1.93521\n",
      "velocity = 0.0426779\n",
      "At minibatch 16700, batch loss 0.026564, batch error rate 0.000000%\n",
      "max norm = 1.94389\n",
      "velocity = 0.0386623\n",
      "At minibatch 16800, batch loss 0.032975, batch error rate 1.000000%\n",
      "max norm = 1.9481\n",
      "velocity = 0.0426991\n",
      "At minibatch 16900, batch loss 0.013151, batch error rate 0.000000%\n",
      "max norm = 1.94819\n",
      "velocity = 0.0486428\n",
      "At minibatch 17000, batch loss 0.017040, batch error rate 1.000000%\n",
      "max norm = 1.94819\n",
      "velocity = 0.0588494\n",
      "After epoch 34: valid_err_rate: 0.018700% currently going ot do 45 epochs\n",
      "At minibatch 17100, batch loss 0.113157, batch error rate 3.000000%\n",
      "max norm = 1.95381\n",
      "velocity = 0.045316\n",
      "At minibatch 17200, batch loss 0.061070, batch error rate 2.000000%\n",
      "max norm = 1.96081\n",
      "velocity = 0.0720115\n",
      "At minibatch 17300, batch loss 0.033407, batch error rate 1.000000%\n",
      "max norm = 1.96094\n",
      "velocity = 0.0554453\n",
      "At minibatch 17400, batch loss 0.025855, batch error rate 0.000000%\n",
      "max norm = 1.96129\n",
      "velocity = 0.0658061\n",
      "At minibatch 17500, batch loss 0.012392, batch error rate 0.000000%\n",
      "max norm = 1.96129\n",
      "velocity = 0.0598687\n",
      "After epoch 35: valid_err_rate: 0.019600% currently going ot do 45 epochs\n",
      "At minibatch 17600, batch loss 0.006817, batch error rate 0.000000%\n",
      "max norm = 1.95519\n",
      "velocity = 0.0392502\n",
      "Setting network parameters from after epoch 34\n",
      "Test error rate: 0.017100\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAFnCAYAAAAGxCvgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noiiKoihRRY0PRVEURVGiyv8DKJhjfUnpv5gAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f56ff1e9850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "# TODO: pick a network architecture here. The one below is just \n",
    "# softmax regression\n",
    "#\n",
    "\n",
    "net = FeedForwardNet([\n",
    "        AffineLayer(784,1000),\n",
    "        ReLULayer(),\n",
    "        AffineLayer(1000, 700),\n",
    "        TanhLayer(),\n",
    "        AffineLayer(700, 10),\n",
    "        SoftMaxLayer()\n",
    "        ])\n",
    "SGD(net, mnist_train_stream, mnist_validation_stream, mnist_test_stream, alpha=-0.1, decay_param=0.001)\n",
    "\n",
    "print \"Test error rate: %f\" % (compute_error_rate(net, mnist_test_stream), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Problem 3 [2p bonus]\n",
    "\n",
    "Implement norm constraints, i.e. limit the total\n",
    "norm of connections incoming to a neuron. In our case, this\n",
    "corresponds to clipping the norm of *rows* of weight\n",
    "matrices. An easy way of implementing it is to make a gradient\n",
    "step, then look at the norm of rows and scale down those that are\n",
    "over the threshold (this technique is called \"projected gradient descent\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4 [2p bonus]\n",
    "\n",
    "Implement a **dropout** layer and try to train a\n",
    "network getting below 1.5% test error rates with dropout (the best\n",
    "result is below 1\\% for dropout!). Details: http://arxiv.org/pdf/1207.0580.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At minibatch 100, batch loss 2.295440, batch error rate 90.000000%\n",
      "max norm = 0.249559\n",
      "velocity = 0.0413448\n",
      "At minibatch 200, batch loss 0.906483, batch error rate 29.000000%\n",
      "max norm = 1.5509\n",
      "velocity = 0.713812\n",
      "At minibatch 300, batch loss 0.513141, batch error rate 16.000000%\n",
      "max norm = 1.92982\n",
      "velocity = 0.606214\n",
      "At minibatch 400, batch loss 0.467235, batch error rate 15.000000%\n",
      "max norm = 2.01053\n",
      "velocity = 0.557671\n",
      "At minibatch 500, batch loss 0.332527, batch error rate 6.000000%\n",
      "max norm = 2.02083\n",
      "velocity = 0.557886\n",
      "After epoch 1: valid_err_rate: 0.074000% currently going ot do 20 epochs\n",
      "At minibatch 600, batch loss 0.369653, batch error rate 10.000000%\n",
      "max norm = 2.02294\n",
      "velocity = 0.661981\n",
      "At minibatch 700, batch loss 0.201460, batch error rate 7.000000%\n",
      "max norm = 2.02725\n",
      "velocity = 0.637954\n",
      "At minibatch 800, batch loss 0.334215, batch error rate 11.000000%\n",
      "max norm = 2.02725\n",
      "velocity = 0.518492\n",
      "At minibatch 900, batch loss 0.425070, batch error rate 12.000000%\n",
      "max norm = 2.02725\n",
      "velocity = 0.5188\n",
      "At minibatch 1000, batch loss 0.280594, batch error rate 8.000000%\n",
      "max norm = 2.02725\n",
      "velocity = 0.495696\n",
      "After epoch 2: valid_err_rate: 0.052200% currently going ot do 20 epochs\n",
      "At minibatch 1100, batch loss 0.423035, batch error rate 10.000000%\n",
      "max norm = 2.01829\n",
      "velocity = 0.507438\n",
      "At minibatch 1200, batch loss 0.167440, batch error rate 7.000000%\n",
      "max norm = 2.02505\n",
      "velocity = 0.477541\n",
      "At minibatch 1300, batch loss 0.173239, batch error rate 6.000000%\n",
      "max norm = 2.02505\n",
      "velocity = 0.487483\n",
      "At minibatch 1400, batch loss 0.348201, batch error rate 10.000000%\n",
      "max norm = 2.02585\n",
      "velocity = 0.58032\n",
      "At minibatch 1500, batch loss 0.189580, batch error rate 6.000000%\n",
      "max norm = 2.02872\n",
      "velocity = 0.561098\n",
      "After epoch 3: valid_err_rate: 0.040100% currently going ot do 20 epochs\n",
      "At minibatch 1600, batch loss 0.339045, batch error rate 7.000000%\n",
      "max norm = 2.02031\n",
      "velocity = 0.484338\n",
      "At minibatch 1700, batch loss 0.180842, batch error rate 6.000000%\n",
      "max norm = 2.02166\n",
      "velocity = 0.532528\n",
      "At minibatch 1800, batch loss 0.254670, batch error rate 8.000000%\n",
      "max norm = 2.02418\n",
      "velocity = 0.496293\n",
      "At minibatch 1900, batch loss 0.130294, batch error rate 4.000000%\n",
      "max norm = 2.0258\n",
      "velocity = 0.543321\n",
      "At minibatch 2000, batch loss 0.195821, batch error rate 6.000000%\n",
      "max norm = 2.0258\n",
      "velocity = 0.492088\n",
      "After epoch 4: valid_err_rate: 0.039700% currently going ot do 20 epochs\n",
      "At minibatch 2100, batch loss 0.168357, batch error rate 5.000000%\n",
      "max norm = 2.0198\n",
      "velocity = 0.517827\n",
      "At minibatch 2200, batch loss 0.133204, batch error rate 6.000000%\n",
      "max norm = 2.03126\n",
      "velocity = 0.570616\n",
      "At minibatch 2300, batch loss 0.382055, batch error rate 7.000000%\n",
      "max norm = 2.03126\n",
      "velocity = 0.504091\n",
      "At minibatch 2400, batch loss 0.192525, batch error rate 6.000000%\n",
      "max norm = 2.03126\n",
      "velocity = 0.423364\n",
      "At minibatch 2500, batch loss 0.477293, batch error rate 12.000000%\n",
      "max norm = 2.03126\n",
      "velocity = 0.492276\n",
      "After epoch 5: valid_err_rate: 0.032700% currently going ot do 20 epochs\n",
      "At minibatch 2600, batch loss 0.117918, batch error rate 4.000000%\n",
      "max norm = 2.02038\n",
      "velocity = 0.501273\n",
      "At minibatch 2700, batch loss 0.078612, batch error rate 2.000000%\n",
      "max norm = 2.02799\n",
      "velocity = 0.378456\n",
      "At minibatch 2800, batch loss 0.158360, batch error rate 8.000000%\n",
      "max norm = 2.02799\n",
      "velocity = 0.491725\n",
      "At minibatch 2900, batch loss 0.217117, batch error rate 7.000000%\n",
      "max norm = 2.02799\n",
      "velocity = 0.49615\n",
      "At minibatch 3000, batch loss 0.062283, batch error rate 1.000000%\n",
      "max norm = 2.02799\n",
      "velocity = 0.504535\n",
      "After epoch 6: valid_err_rate: 0.032600% currently going ot do 20 epochs\n",
      "At minibatch 3100, batch loss 0.138656, batch error rate 5.000000%\n",
      "max norm = 2.02238\n",
      "velocity = 0.516901\n",
      "At minibatch 3200, batch loss 0.166600, batch error rate 6.000000%\n",
      "max norm = 2.02238\n",
      "velocity = 0.480802\n",
      "At minibatch 3300, batch loss 0.136622, batch error rate 7.000000%\n",
      "max norm = 2.02238\n",
      "velocity = 0.413276\n",
      "At minibatch 3400, batch loss 0.119460, batch error rate 5.000000%\n",
      "max norm = 2.02238\n",
      "velocity = 0.542884\n",
      "At minibatch 3500, batch loss 0.195217, batch error rate 8.000000%\n",
      "max norm = 2.0256\n",
      "velocity = 0.515169\n",
      "After epoch 7: valid_err_rate: 0.033800% currently going ot do 20 epochs\n",
      "At minibatch 3600, batch loss 0.166482, batch error rate 5.000000%\n",
      "max norm = 2.02724\n",
      "velocity = 0.416559\n",
      "At minibatch 3700, batch loss 0.141773, batch error rate 5.000000%\n",
      "max norm = 2.02724\n",
      "velocity = 0.493028\n",
      "At minibatch 3800, batch loss 0.150309, batch error rate 4.000000%\n",
      "max norm = 2.02724\n",
      "velocity = 0.395546\n",
      "At minibatch 3900, batch loss 0.127646, batch error rate 4.000000%\n",
      "max norm = 2.02724\n",
      "velocity = 0.421658\n",
      "At minibatch 4000, batch loss 0.236330, batch error rate 8.000000%\n",
      "max norm = 2.02724\n",
      "velocity = 0.430376\n",
      "After epoch 8: valid_err_rate: 0.034100% currently going ot do 20 epochs\n",
      "At minibatch 4100, batch loss 0.163478, batch error rate 5.000000%\n",
      "max norm = 2.02374\n",
      "velocity = 0.391621\n",
      "At minibatch 4200, batch loss 0.136945, batch error rate 4.000000%\n",
      "max norm = 2.02374\n",
      "velocity = 0.492265\n",
      "At minibatch 4300, batch loss 0.180296, batch error rate 3.000000%\n",
      "max norm = 2.02374\n",
      "velocity = 0.463008\n",
      "At minibatch 4400, batch loss 0.134591, batch error rate 3.000000%\n",
      "max norm = 2.02374\n",
      "velocity = 0.532567\n",
      "At minibatch 4500, batch loss 0.131136, batch error rate 2.000000%\n",
      "max norm = 2.02374\n",
      "velocity = 0.52903\n",
      "After epoch 9: valid_err_rate: 0.032000% currently going ot do 20 epochs\n",
      "At minibatch 4600, batch loss 0.184816, batch error rate 4.000000%\n",
      "max norm = 2.02306\n",
      "velocity = 0.386428\n",
      "At minibatch 4700, batch loss 0.185652, batch error rate 6.000000%\n",
      "max norm = 2.02703\n",
      "velocity = 0.369503\n",
      "At minibatch 4800, batch loss 0.165625, batch error rate 6.000000%\n",
      "max norm = 2.02703\n",
      "velocity = 0.471306\n",
      "At minibatch 4900, batch loss 0.109483, batch error rate 4.000000%\n",
      "max norm = 2.0286\n",
      "velocity = 0.564774\n",
      "At minibatch 5000, batch loss 0.182327, batch error rate 5.000000%\n",
      "max norm = 2.0286\n",
      "velocity = 0.443725\n",
      "After epoch 10: valid_err_rate: 0.028300% currently going ot do 20 epochs\n",
      "At minibatch 5100, batch loss 0.125498, batch error rate 3.000000%\n",
      "max norm = 2.0157\n",
      "velocity = 0.291981\n",
      "At minibatch 5200, batch loss 0.058452, batch error rate 2.000000%\n",
      "max norm = 2.0157\n",
      "velocity = 0.257703\n",
      "At minibatch 5300, batch loss 0.095565, batch error rate 4.000000%\n",
      "max norm = 2.0157\n",
      "velocity = 0.31389\n",
      "At minibatch 5400, batch loss 0.051197, batch error rate 3.000000%\n",
      "max norm = 2.0157\n",
      "velocity = 0.309494\n",
      "At minibatch 5500, batch loss 0.093107, batch error rate 3.000000%\n",
      "max norm = 2.0157\n",
      "velocity = 0.311831\n",
      "After epoch 11: valid_err_rate: 0.024200% currently going ot do 20 epochs\n",
      "At minibatch 5600, batch loss 0.078821, batch error rate 4.000000%\n",
      "max norm = 2.01328\n",
      "velocity = 0.250674\n",
      "At minibatch 5700, batch loss 0.086772, batch error rate 4.000000%\n",
      "max norm = 2.01547\n",
      "velocity = 0.321289\n",
      "At minibatch 5800, batch loss 0.045646, batch error rate 2.000000%\n",
      "max norm = 2.01547\n",
      "velocity = 0.271329\n",
      "At minibatch 5900, batch loss 0.131523, batch error rate 6.000000%\n",
      "max norm = 2.01547\n",
      "velocity = 0.236936\n",
      "At minibatch 6000, batch loss 0.053741, batch error rate 3.000000%\n",
      "max norm = 2.01547\n",
      "velocity = 0.3073\n",
      "After epoch 12: valid_err_rate: 0.026200% currently going ot do 20 epochs\n",
      "At minibatch 6100, batch loss 0.032538, batch error rate 1.000000%\n",
      "max norm = 2.01427\n",
      "velocity = 0.241014\n",
      "At minibatch 6200, batch loss 0.174713, batch error rate 3.000000%\n",
      "max norm = 2.01427\n",
      "velocity = 0.329077\n",
      "At minibatch 6300, batch loss 0.167178, batch error rate 5.000000%\n",
      "max norm = 2.02189\n",
      "velocity = 0.333085\n",
      "At minibatch 6400, batch loss 0.164253, batch error rate 3.000000%\n",
      "max norm = 2.02189\n",
      "velocity = 0.279773\n",
      "At minibatch 6500, batch loss 0.038320, batch error rate 2.000000%\n",
      "max norm = 2.02189\n",
      "velocity = 0.228034\n",
      "After epoch 13: valid_err_rate: 0.021700% currently going ot do 20 epochs\n",
      "At minibatch 6600, batch loss 0.098406, batch error rate 3.000000%\n",
      "max norm = 2.01008\n",
      "velocity = 0.291413\n",
      "At minibatch 6700, batch loss 0.041951, batch error rate 1.000000%\n",
      "max norm = 2.01466\n",
      "velocity = 0.293426\n",
      "At minibatch 6800, batch loss 0.058742, batch error rate 1.000000%\n",
      "max norm = 2.01466\n",
      "velocity = 0.290924\n",
      "At minibatch 6900, batch loss 0.093935, batch error rate 4.000000%\n",
      "max norm = 2.01466\n",
      "velocity = 0.264646\n",
      "At minibatch 7000, batch loss 0.089416, batch error rate 2.000000%\n",
      "max norm = 2.01466\n",
      "velocity = 0.252095\n",
      "After epoch 14: valid_err_rate: 0.024000% currently going ot do 20 epochs\n",
      "At minibatch 7100, batch loss 0.027422, batch error rate 1.000000%\n",
      "max norm = 2.01337\n",
      "velocity = 0.266906\n",
      "At minibatch 7200, batch loss 0.054970, batch error rate 2.000000%\n",
      "max norm = 2.01617\n",
      "velocity = 0.248646\n",
      "At minibatch 7300, batch loss 0.010856, batch error rate 0.000000%\n",
      "max norm = 2.01617\n",
      "velocity = 0.319301\n",
      "At minibatch 7400, batch loss 0.070298, batch error rate 2.000000%\n",
      "max norm = 2.01617\n",
      "velocity = 0.275835\n",
      "At minibatch 7500, batch loss 0.176211, batch error rate 3.000000%\n",
      "max norm = 2.01617\n",
      "velocity = 0.255207\n",
      "After epoch 15: valid_err_rate: 0.023500% currently going ot do 20 epochs\n",
      "At minibatch 7600, batch loss 0.076658, batch error rate 3.000000%\n",
      "max norm = 2.01443\n",
      "velocity = 0.272105\n",
      "At minibatch 7700, batch loss 0.021024, batch error rate 0.000000%\n",
      "max norm = 2.01443\n",
      "velocity = 0.303467\n",
      "At minibatch 7800, batch loss 0.087564, batch error rate 2.000000%\n",
      "max norm = 2.01443\n",
      "velocity = 0.267035\n",
      "At minibatch 7900, batch loss 0.127171, batch error rate 4.000000%\n",
      "max norm = 2.02187\n",
      "velocity = 0.280523\n",
      "At minibatch 8000, batch loss 0.070246, batch error rate 2.000000%\n",
      "max norm = 2.02187\n",
      "velocity = 0.27614\n",
      "After epoch 16: valid_err_rate: 0.021700% currently going ot do 20 epochs\n",
      "At minibatch 8100, batch loss 0.147322, batch error rate 4.000000%\n",
      "max norm = 2.01666\n",
      "velocity = 0.342337\n",
      "At minibatch 8200, batch loss 0.041520, batch error rate 3.000000%\n",
      "max norm = 2.01666\n",
      "velocity = 0.208648\n",
      "At minibatch 8300, batch loss 0.073415, batch error rate 3.000000%\n",
      "max norm = 2.01666\n",
      "velocity = 0.251184\n",
      "At minibatch 8400, batch loss 0.044409, batch error rate 1.000000%\n",
      "max norm = 2.01666\n",
      "velocity = 0.270015\n",
      "At minibatch 8500, batch loss 0.118470, batch error rate 4.000000%\n",
      "max norm = 2.01666\n",
      "velocity = 0.280236\n",
      "After epoch 17: valid_err_rate: 0.020700% currently going ot do 23 epochs\n",
      "At minibatch 8600, batch loss 0.121894, batch error rate 3.000000%\n",
      "max norm = 2.01557\n",
      "velocity = 0.236303\n",
      "At minibatch 8700, batch loss 0.070873, batch error rate 4.000000%\n",
      "max norm = 2.01557\n",
      "velocity = 0.257005\n",
      "At minibatch 8800, batch loss 0.065465, batch error rate 1.000000%\n",
      "max norm = 2.01557\n",
      "velocity = 0.310876\n",
      "At minibatch 8900, batch loss 0.016011, batch error rate 1.000000%\n",
      "max norm = 2.01557\n",
      "velocity = 0.285667\n",
      "At minibatch 9000, batch loss 0.074015, batch error rate 2.000000%\n",
      "max norm = 2.01557\n",
      "velocity = 0.348053\n",
      "After epoch 18: valid_err_rate: 0.022200% currently going ot do 23 epochs\n",
      "At minibatch 9100, batch loss 0.036381, batch error rate 0.000000%\n",
      "max norm = 2.017\n",
      "velocity = 0.227852\n",
      "At minibatch 9200, batch loss 0.077178, batch error rate 4.000000%\n",
      "max norm = 2.01754\n",
      "velocity = 0.288335\n",
      "At minibatch 9300, batch loss 0.039621, batch error rate 1.000000%\n",
      "max norm = 2.01754\n",
      "velocity = 0.311443\n",
      "At minibatch 9400, batch loss 0.081513, batch error rate 2.000000%\n",
      "max norm = 2.01754\n",
      "velocity = 0.277732\n",
      "At minibatch 9500, batch loss 0.286220, batch error rate 7.000000%\n",
      "max norm = 2.01754\n",
      "velocity = 0.201553\n",
      "After epoch 19: valid_err_rate: 0.021000% currently going ot do 23 epochs\n",
      "At minibatch 9600, batch loss 0.032920, batch error rate 1.000000%\n",
      "max norm = 2.01483\n",
      "velocity = 0.301916\n",
      "At minibatch 9700, batch loss 0.008001, batch error rate 0.000000%\n",
      "max norm = 2.01483\n",
      "velocity = 0.297203\n",
      "At minibatch 9800, batch loss 0.171749, batch error rate 5.000000%\n",
      "max norm = 2.01483\n",
      "velocity = 0.222325\n",
      "At minibatch 9900, batch loss 0.016818, batch error rate 0.000000%\n",
      "max norm = 2.01483\n",
      "velocity = 0.235817\n",
      "At minibatch 10000, batch loss 0.029751, batch error rate 0.000000%\n",
      "max norm = 2.01483\n",
      "velocity = 0.230543\n",
      "After epoch 20: valid_err_rate: 0.022900% currently going ot do 23 epochs\n",
      "At minibatch 10100, batch loss 0.024702, batch error rate 1.000000%\n",
      "max norm = 2.00891\n",
      "velocity = 0.186235\n",
      "At minibatch 10200, batch loss 0.012010, batch error rate 0.000000%\n",
      "max norm = 2.00898\n",
      "velocity = 0.213213\n",
      "At minibatch 10300, batch loss 0.043832, batch error rate 1.000000%\n",
      "max norm = 2.00898\n",
      "velocity = 0.122129\n",
      "At minibatch 10400, batch loss 0.039131, batch error rate 1.000000%\n",
      "max norm = 2.00898\n",
      "velocity = 0.190065\n",
      "At minibatch 10500, batch loss 0.017068, batch error rate 1.000000%\n",
      "max norm = 2.00898\n",
      "velocity = 0.199717\n",
      "After epoch 21: valid_err_rate: 0.018500% currently going ot do 28 epochs\n",
      "At minibatch 10600, batch loss 0.016334, batch error rate 0.000000%\n",
      "max norm = 2.00822\n",
      "velocity = 0.160851\n",
      "At minibatch 10700, batch loss 0.044670, batch error rate 3.000000%\n",
      "max norm = 2.00822\n",
      "velocity = 0.196714\n",
      "At minibatch 10800, batch loss 0.077610, batch error rate 2.000000%\n",
      "max norm = 2.00955\n",
      "velocity = 0.175715\n",
      "At minibatch 10900, batch loss 0.088695, batch error rate 2.000000%\n",
      "max norm = 2.01484\n",
      "velocity = 0.117127\n",
      "At minibatch 11000, batch loss 0.050547, batch error rate 4.000000%\n",
      "max norm = 2.01484\n",
      "velocity = 0.1558\n",
      "After epoch 22: valid_err_rate: 0.020100% currently going ot do 28 epochs\n",
      "At minibatch 11100, batch loss 0.034138, batch error rate 0.000000%\n",
      "max norm = 2.00672\n",
      "velocity = 0.126756\n",
      "At minibatch 11200, batch loss 0.064695, batch error rate 2.000000%\n",
      "max norm = 2.01075\n",
      "velocity = 0.138708\n",
      "At minibatch 11300, batch loss 0.025377, batch error rate 0.000000%\n",
      "max norm = 2.01107\n",
      "velocity = 0.111528\n",
      "At minibatch 11400, batch loss 0.026528, batch error rate 1.000000%\n",
      "max norm = 2.01107\n",
      "velocity = 0.139945\n",
      "At minibatch 11500, batch loss 0.071374, batch error rate 2.000000%\n",
      "max norm = 2.01107\n",
      "velocity = 0.146223\n",
      "After epoch 23: valid_err_rate: 0.020800% currently going ot do 28 epochs\n",
      "At minibatch 11600, batch loss 0.022162, batch error rate 1.000000%\n",
      "max norm = 2.00961\n",
      "velocity = 0.193094\n",
      "At minibatch 11700, batch loss 0.041215, batch error rate 1.000000%\n",
      "max norm = 2.0131\n",
      "velocity = 0.123448\n",
      "At minibatch 11800, batch loss 0.085813, batch error rate 2.000000%\n",
      "max norm = 2.0131\n",
      "velocity = 0.101336\n",
      "At minibatch 11900, batch loss 0.004207, batch error rate 0.000000%\n",
      "max norm = 2.0131\n",
      "velocity = 0.126193\n",
      "At minibatch 12000, batch loss 0.014809, batch error rate 0.000000%\n",
      "max norm = 2.0131\n",
      "velocity = 0.228975\n",
      "After epoch 24: valid_err_rate: 0.019600% currently going ot do 28 epochs\n",
      "At minibatch 12100, batch loss 0.038290, batch error rate 1.000000%\n",
      "max norm = 2.00828\n",
      "velocity = 0.156719\n",
      "At minibatch 12200, batch loss 0.073048, batch error rate 1.000000%\n",
      "max norm = 2.01047\n",
      "velocity = 0.170569\n",
      "At minibatch 12300, batch loss 0.027985, batch error rate 1.000000%\n",
      "max norm = 2.0107\n",
      "velocity = 0.149725\n",
      "At minibatch 12400, batch loss 0.078688, batch error rate 2.000000%\n",
      "max norm = 2.01163\n",
      "velocity = 0.144249\n",
      "At minibatch 12500, batch loss 0.080692, batch error rate 4.000000%\n",
      "max norm = 2.01163\n",
      "velocity = 0.158383\n",
      "After epoch 25: valid_err_rate: 0.020400% currently going ot do 28 epochs\n",
      "At minibatch 12600, batch loss 0.103847, batch error rate 2.000000%\n",
      "max norm = 2.0074\n",
      "velocity = 0.114459\n",
      "At minibatch 12700, batch loss 0.149239, batch error rate 6.000000%\n",
      "max norm = 2.00844\n",
      "velocity = 0.138779\n",
      "At minibatch 12800, batch loss 0.022311, batch error rate 0.000000%\n",
      "max norm = 2.01007\n",
      "velocity = 0.16441\n",
      "At minibatch 12900, batch loss 0.135469, batch error rate 3.000000%\n",
      "max norm = 2.01009\n",
      "velocity = 0.145354\n",
      "At minibatch 13000, batch loss 0.054889, batch error rate 1.000000%\n",
      "max norm = 2.01009\n",
      "velocity = 0.168879\n",
      "After epoch 26: valid_err_rate: 0.017800% currently going ot do 34 epochs\n",
      "At minibatch 13100, batch loss 0.018298, batch error rate 0.000000%\n",
      "max norm = 2.0067\n",
      "velocity = 0.144914\n",
      "At minibatch 13200, batch loss 0.012505, batch error rate 0.000000%\n",
      "max norm = 2.0067\n",
      "velocity = 0.151239\n",
      "At minibatch 13300, batch loss 0.008676, batch error rate 0.000000%\n",
      "max norm = 2.00849\n",
      "velocity = 0.169136\n",
      "At minibatch 13400, batch loss 0.012878, batch error rate 1.000000%\n",
      "max norm = 2.01107\n",
      "velocity = 0.132556\n",
      "At minibatch 13500, batch loss 0.044053, batch error rate 2.000000%\n",
      "max norm = 2.0113\n",
      "velocity = 0.108087\n",
      "After epoch 27: valid_err_rate: 0.020200% currently going ot do 34 epochs\n",
      "At minibatch 13600, batch loss 0.023186, batch error rate 1.000000%\n",
      "max norm = 2.00923\n",
      "velocity = 0.195644\n",
      "At minibatch 13700, batch loss 0.088938, batch error rate 2.000000%\n",
      "max norm = 2.00923\n",
      "velocity = 0.155982\n",
      "At minibatch 13800, batch loss 0.103956, batch error rate 2.000000%\n",
      "max norm = 2.00923\n",
      "velocity = 0.135687\n",
      "At minibatch 13900, batch loss 0.023647, batch error rate 0.000000%\n",
      "max norm = 2.00923\n",
      "velocity = 0.147355\n",
      "At minibatch 14000, batch loss 0.084984, batch error rate 4.000000%\n",
      "max norm = 2.01018\n",
      "velocity = 0.167636\n",
      "After epoch 28: valid_err_rate: 0.018800% currently going ot do 34 epochs\n",
      "At minibatch 14100, batch loss 0.014766, batch error rate 1.000000%\n",
      "max norm = 2.00651\n",
      "velocity = 0.158874\n",
      "At minibatch 14200, batch loss 0.084472, batch error rate 3.000000%\n",
      "max norm = 2.00838\n",
      "velocity = 0.188967\n",
      "At minibatch 14300, batch loss 0.056511, batch error rate 2.000000%\n",
      "max norm = 2.01217\n",
      "velocity = 0.150333\n",
      "At minibatch 14400, batch loss 0.002272, batch error rate 0.000000%\n",
      "max norm = 2.01217\n",
      "velocity = 0.179992\n",
      "At minibatch 14500, batch loss 0.033892, batch error rate 1.000000%\n",
      "max norm = 2.01217\n",
      "velocity = 0.204071\n",
      "After epoch 29: valid_err_rate: 0.018200% currently going ot do 34 epochs\n",
      "At minibatch 14600, batch loss 0.101188, batch error rate 2.000000%\n",
      "max norm = 2.00821\n",
      "velocity = 0.126364\n",
      "At minibatch 14700, batch loss 0.044851, batch error rate 3.000000%\n",
      "max norm = 2.0084\n",
      "velocity = 0.129514\n",
      "At minibatch 14800, batch loss 0.070092, batch error rate 2.000000%\n",
      "max norm = 2.0084\n",
      "velocity = 0.194882\n",
      "At minibatch 14900, batch loss 0.038119, batch error rate 1.000000%\n",
      "max norm = 2.00952\n",
      "velocity = 0.139226\n",
      "At minibatch 15000, batch loss 0.020074, batch error rate 1.000000%\n",
      "max norm = 2.00952\n",
      "velocity = 0.166545\n",
      "After epoch 30: valid_err_rate: 0.019800% currently going ot do 34 epochs\n",
      "At minibatch 15100, batch loss 0.007757, batch error rate 0.000000%\n",
      "max norm = 2.00427\n",
      "velocity = 0.133094\n",
      "At minibatch 15200, batch loss 0.025545, batch error rate 0.000000%\n",
      "max norm = 2.00427\n",
      "velocity = 0.0816628\n",
      "At minibatch 15300, batch loss 0.016304, batch error rate 0.000000%\n",
      "max norm = 2.00593\n",
      "velocity = 0.0691538\n",
      "At minibatch 15400, batch loss 0.049243, batch error rate 2.000000%\n",
      "max norm = 2.00593\n",
      "velocity = 0.103742\n",
      "At minibatch 15500, batch loss 0.017967, batch error rate 1.000000%\n",
      "max norm = 2.00593\n",
      "velocity = 0.113896\n",
      "After epoch 31: valid_err_rate: 0.017400% currently going ot do 41 epochs\n",
      "At minibatch 15600, batch loss 0.055201, batch error rate 2.000000%\n",
      "max norm = 2.00457\n",
      "velocity = 0.0740981\n",
      "At minibatch 15700, batch loss 0.040490, batch error rate 3.000000%\n",
      "max norm = 2.00655\n",
      "velocity = 0.096226\n",
      "At minibatch 15800, batch loss 0.056614, batch error rate 2.000000%\n",
      "max norm = 2.00655\n",
      "velocity = 0.102293\n",
      "At minibatch 15900, batch loss 0.006413, batch error rate 0.000000%\n",
      "max norm = 2.00655\n",
      "velocity = 0.084199\n",
      "At minibatch 16000, batch loss 0.008699, batch error rate 0.000000%\n",
      "max norm = 2.00655\n",
      "velocity = 0.0946502\n",
      "After epoch 32: valid_err_rate: 0.016100% currently going ot do 42 epochs\n",
      "At minibatch 16100, batch loss 0.015440, batch error rate 0.000000%\n",
      "max norm = 2.00359\n",
      "velocity = 0.0638115\n",
      "At minibatch 16200, batch loss 0.006132, batch error rate 0.000000%\n",
      "max norm = 2.00541\n",
      "velocity = 0.108553\n",
      "At minibatch 16300, batch loss 0.042704, batch error rate 1.000000%\n",
      "max norm = 2.00541\n",
      "velocity = 0.0788662\n",
      "At minibatch 16400, batch loss 0.010695, batch error rate 1.000000%\n",
      "max norm = 2.00608\n",
      "velocity = 0.0780904\n",
      "At minibatch 16500, batch loss 0.067215, batch error rate 3.000000%\n",
      "max norm = 2.00608\n",
      "velocity = 0.107499\n",
      "After epoch 33: valid_err_rate: 0.017100% currently going ot do 42 epochs\n",
      "At minibatch 16600, batch loss 0.056205, batch error rate 3.000000%\n",
      "max norm = 2.00468\n",
      "velocity = 0.1056\n",
      "At minibatch 16700, batch loss 0.021234, batch error rate 1.000000%\n",
      "max norm = 2.00551\n",
      "velocity = 0.0654285\n",
      "At minibatch 16800, batch loss 0.005336, batch error rate 0.000000%\n",
      "max norm = 2.00551\n",
      "velocity = 0.0782398\n",
      "At minibatch 16900, batch loss 0.003513, batch error rate 0.000000%\n",
      "max norm = 2.00551\n",
      "velocity = 0.0823142\n",
      "At minibatch 17000, batch loss 0.027956, batch error rate 1.000000%\n",
      "max norm = 2.00551\n",
      "velocity = 0.0871108\n",
      "After epoch 34: valid_err_rate: 0.015400% currently going ot do 45 epochs\n",
      "At minibatch 17100, batch loss 0.019464, batch error rate 1.000000%\n",
      "max norm = 2.00374\n",
      "velocity = 0.0778948\n",
      "At minibatch 17200, batch loss 0.076763, batch error rate 4.000000%\n",
      "max norm = 2.005\n",
      "velocity = 0.0767764\n",
      "At minibatch 17300, batch loss 0.027805, batch error rate 1.000000%\n",
      "max norm = 2.00541\n",
      "velocity = 0.0893429\n",
      "At minibatch 17400, batch loss 0.008824, batch error rate 0.000000%\n",
      "max norm = 2.00541\n",
      "velocity = 0.0888726\n",
      "At minibatch 17500, batch loss 0.002436, batch error rate 0.000000%\n",
      "max norm = 2.00672\n",
      "velocity = 0.118778\n",
      "After epoch 35: valid_err_rate: 0.016100% currently going ot do 45 epochs\n",
      "At minibatch 17600, batch loss 0.020752, batch error rate 1.000000%\n",
      "max norm = 2.00454\n",
      "velocity = 0.0953473\n",
      "At minibatch 17700, batch loss 0.002398, batch error rate 0.000000%\n",
      "max norm = 2.00454\n",
      "velocity = 0.0875561\n",
      "At minibatch 17800, batch loss 0.045037, batch error rate 1.000000%\n",
      "max norm = 2.00454\n",
      "velocity = 0.160145\n",
      "At minibatch 17900, batch loss 0.034607, batch error rate 2.000000%\n",
      "max norm = 2.00454\n",
      "velocity = 0.109994\n",
      "At minibatch 18000, batch loss 0.036034, batch error rate 1.000000%\n",
      "max norm = 2.00454\n",
      "velocity = 0.0865586\n",
      "After epoch 36: valid_err_rate: 0.015700% currently going ot do 45 epochs\n",
      "At minibatch 18100, batch loss 0.054035, batch error rate 2.000000%\n",
      "max norm = 2.00559\n",
      "velocity = 0.105103\n",
      "At minibatch 18200, batch loss 0.011702, batch error rate 0.000000%\n",
      "max norm = 2.00559\n",
      "velocity = 0.106077\n",
      "At minibatch 18300, batch loss 0.018557, batch error rate 1.000000%\n",
      "max norm = 2.00574\n",
      "velocity = 0.111936\n",
      "At minibatch 18400, batch loss 0.003331, batch error rate 0.000000%\n",
      "max norm = 2.00655\n",
      "velocity = 0.0774447\n",
      "At minibatch 18500, batch loss 0.028367, batch error rate 0.000000%\n",
      "max norm = 2.00655\n",
      "velocity = 0.0882645\n",
      "After epoch 37: valid_err_rate: 0.016200% currently going ot do 45 epochs\n",
      "At minibatch 18600, batch loss 0.001324, batch error rate 0.000000%\n",
      "max norm = 2.0045\n",
      "velocity = 0.0867305\n",
      "At minibatch 18700, batch loss 0.022426, batch error rate 1.000000%\n",
      "max norm = 2.00525\n",
      "velocity = 0.0987447\n",
      "At minibatch 18800, batch loss 0.134357, batch error rate 2.000000%\n",
      "max norm = 2.00525\n",
      "velocity = 0.068779\n",
      "At minibatch 18900, batch loss 0.056759, batch error rate 2.000000%\n",
      "max norm = 2.00525\n",
      "velocity = 0.0842536\n",
      "At minibatch 19000, batch loss 0.003378, batch error rate 0.000000%\n",
      "max norm = 2.00525\n",
      "velocity = 0.0836124\n",
      "After epoch 38: valid_err_rate: 0.015800% currently going ot do 45 epochs\n",
      "At minibatch 19100, batch loss 0.010055, batch error rate 0.000000%\n",
      "max norm = 2.00361\n",
      "velocity = 0.0577815\n",
      "At minibatch 19200, batch loss 0.009585, batch error rate 0.000000%\n",
      "max norm = 2.00458\n",
      "velocity = 0.111276\n",
      "At minibatch 19300, batch loss 0.019946, batch error rate 1.000000%\n",
      "max norm = 2.00458\n",
      "velocity = 0.076221\n",
      "At minibatch 19400, batch loss 0.007804, batch error rate 0.000000%\n",
      "max norm = 2.00585\n",
      "velocity = 0.110288\n",
      "At minibatch 19500, batch loss 0.009935, batch error rate 0.000000%\n",
      "max norm = 2.00678\n",
      "velocity = 0.0846022\n",
      "After epoch 39: valid_err_rate: 0.016100% currently going ot do 45 epochs\n",
      "At minibatch 19600, batch loss 0.015230, batch error rate 0.000000%\n",
      "max norm = 2.0049\n",
      "velocity = 0.0634158\n",
      "At minibatch 19700, batch loss 0.009254, batch error rate 0.000000%\n",
      "max norm = 2.00551\n",
      "velocity = 0.0974039\n",
      "At minibatch 19800, batch loss 0.011828, batch error rate 0.000000%\n",
      "max norm = 2.00551\n",
      "velocity = 0.0509841\n",
      "At minibatch 19900, batch loss 0.014413, batch error rate 1.000000%\n",
      "max norm = 2.00551\n",
      "velocity = 0.0775071\n",
      "At minibatch 20000, batch loss 0.010227, batch error rate 0.000000%\n",
      "max norm = 2.00551\n",
      "velocity = 0.0713703\n",
      "After epoch 40: valid_err_rate: 0.017000% currently going ot do 45 epochs\n",
      "At minibatch 20100, batch loss 0.019401, batch error rate 1.000000%\n",
      "max norm = 2.00337\n",
      "velocity = 0.0462296\n",
      "At minibatch 20200, batch loss 0.015022, batch error rate 1.000000%\n",
      "max norm = 2.00337\n",
      "velocity = 0.0741919\n",
      "At minibatch 20300, batch loss 0.017387, batch error rate 1.000000%\n",
      "max norm = 2.00337\n",
      "velocity = 0.0585534\n",
      "At minibatch 20400, batch loss 0.029941, batch error rate 1.000000%\n",
      "max norm = 2.00337\n",
      "velocity = 0.0459174\n",
      "At minibatch 20500, batch loss 0.025250, batch error rate 1.000000%\n",
      "max norm = 2.00337\n",
      "velocity = 0.0493908\n",
      "After epoch 41: valid_err_rate: 0.015600% currently going ot do 45 epochs\n",
      "At minibatch 20600, batch loss 0.056968, batch error rate 1.000000%\n",
      "max norm = 2.00399\n",
      "velocity = 0.0553043\n",
      "At minibatch 20700, batch loss 0.003876, batch error rate 0.000000%\n",
      "max norm = 2.00441\n",
      "velocity = 0.0449364\n",
      "At minibatch 20800, batch loss 0.010561, batch error rate 0.000000%\n",
      "max norm = 2.00441\n",
      "velocity = 0.0476536\n",
      "At minibatch 20900, batch loss 0.009617, batch error rate 0.000000%\n",
      "max norm = 2.00441\n",
      "velocity = 0.0299395\n",
      "At minibatch 21000, batch loss 0.003928, batch error rate 0.000000%\n",
      "max norm = 2.00449\n",
      "velocity = 0.0463353\n",
      "After epoch 42: valid_err_rate: 0.015800% currently going ot do 45 epochs\n",
      "At minibatch 21100, batch loss 0.005073, batch error rate 0.000000%\n",
      "max norm = 2.00271\n",
      "velocity = 0.0453536\n",
      "At minibatch 21200, batch loss 0.068044, batch error rate 3.000000%\n",
      "max norm = 2.00336\n",
      "velocity = 0.0555252\n",
      "At minibatch 21300, batch loss 0.011415, batch error rate 0.000000%\n",
      "max norm = 2.0036\n",
      "velocity = 0.0608743\n",
      "At minibatch 21400, batch loss 0.077426, batch error rate 3.000000%\n",
      "max norm = 2.00377\n",
      "velocity = 0.0598709\n",
      "At minibatch 21500, batch loss 0.002528, batch error rate 0.000000%\n",
      "max norm = 2.00377\n",
      "velocity = 0.0854144\n",
      "After epoch 43: valid_err_rate: 0.015900% currently going ot do 45 epochs\n",
      "At minibatch 21600, batch loss 0.078041, batch error rate 3.000000%\n",
      "max norm = 2.00285\n",
      "velocity = 0.0428429\n",
      "At minibatch 21700, batch loss 0.014726, batch error rate 1.000000%\n",
      "max norm = 2.00306\n",
      "velocity = 0.0485214\n",
      "At minibatch 21800, batch loss 0.004096, batch error rate 0.000000%\n",
      "max norm = 2.00306\n",
      "velocity = 0.0338503\n",
      "At minibatch 21900, batch loss 0.005137, batch error rate 0.000000%\n",
      "max norm = 2.00306\n",
      "velocity = 0.0474854\n",
      "At minibatch 22000, batch loss 0.006262, batch error rate 0.000000%\n",
      "max norm = 2.00424\n",
      "velocity = 0.0626448\n",
      "After epoch 44: valid_err_rate: 0.014900% currently going ot do 58 epochs\n",
      "At minibatch 22100, batch loss 0.019509, batch error rate 1.000000%\n",
      "max norm = 2.00346\n",
      "velocity = 0.0570849\n",
      "At minibatch 22200, batch loss 0.002372, batch error rate 0.000000%\n",
      "max norm = 2.00412\n",
      "velocity = 0.04518\n",
      "At minibatch 22300, batch loss 0.026568, batch error rate 1.000000%\n",
      "max norm = 2.00412\n",
      "velocity = 0.0310377\n",
      "At minibatch 22400, batch loss 0.026905, batch error rate 1.000000%\n",
      "max norm = 2.00412\n",
      "velocity = 0.0552579\n",
      "At minibatch 22500, batch loss 0.027154, batch error rate 1.000000%\n",
      "max norm = 2.00412\n",
      "velocity = 0.0471739\n",
      "After epoch 45: valid_err_rate: 0.015700% currently going ot do 58 epochs\n",
      "At minibatch 22600, batch loss 0.007957, batch error rate 0.000000%\n",
      "max norm = 2.00367\n",
      "velocity = 0.0571238\n",
      "At minibatch 22700, batch loss 0.010629, batch error rate 0.000000%\n",
      "max norm = 2.00367\n",
      "velocity = 0.0468032\n",
      "At minibatch 22800, batch loss 0.053584, batch error rate 1.000000%\n",
      "max norm = 2.00367\n",
      "velocity = 0.0616255\n",
      "At minibatch 22900, batch loss 0.010587, batch error rate 1.000000%\n",
      "max norm = 2.00377\n",
      "velocity = 0.0618028\n",
      "At minibatch 23000, batch loss 0.022839, batch error rate 0.000000%\n",
      "max norm = 2.00377\n",
      "velocity = 0.0225925\n",
      "After epoch 46: valid_err_rate: 0.016700% currently going ot do 58 epochs\n",
      "At minibatch 23100, batch loss 0.002856, batch error rate 0.000000%\n",
      "max norm = 2.00229\n",
      "velocity = 0.0450491\n",
      "At minibatch 23200, batch loss 0.028949, batch error rate 1.000000%\n",
      "max norm = 2.00284\n",
      "velocity = 0.0841223\n",
      "At minibatch 23300, batch loss 0.007222, batch error rate 0.000000%\n",
      "max norm = 2.00386\n",
      "velocity = 0.0535873\n",
      "At minibatch 23400, batch loss 0.020894, batch error rate 1.000000%\n",
      "max norm = 2.00386\n",
      "velocity = 0.0590656\n",
      "At minibatch 23500, batch loss 0.002494, batch error rate 0.000000%\n",
      "max norm = 2.00386\n",
      "velocity = 0.0706243\n",
      "After epoch 47: valid_err_rate: 0.015800% currently going ot do 58 epochs\n",
      "At minibatch 23600, batch loss 0.005838, batch error rate 0.000000%\n",
      "max norm = 2.00354\n",
      "velocity = 0.0464484\n",
      "At minibatch 23700, batch loss 0.028503, batch error rate 1.000000%\n",
      "max norm = 2.00354\n",
      "velocity = 0.0394828\n",
      "At minibatch 23800, batch loss 0.001816, batch error rate 0.000000%\n",
      "max norm = 2.00354\n",
      "velocity = 0.0458127\n",
      "At minibatch 23900, batch loss 0.010261, batch error rate 0.000000%\n",
      "max norm = 2.00354\n",
      "velocity = 0.0488067\n",
      "At minibatch 24000, batch loss 0.062871, batch error rate 2.000000%\n",
      "max norm = 2.00395\n",
      "velocity = 0.0687824\n",
      "After epoch 48: valid_err_rate: 0.016300% currently going ot do 58 epochs\n",
      "At minibatch 24100, batch loss 0.004956, batch error rate 0.000000%\n",
      "max norm = 2.00411\n",
      "velocity = 0.0664524\n",
      "At minibatch 24200, batch loss 0.006638, batch error rate 0.000000%\n",
      "max norm = 2.00411\n",
      "velocity = 0.0539147\n",
      "At minibatch 24300, batch loss 0.022515, batch error rate 1.000000%\n",
      "max norm = 2.00607\n",
      "velocity = 0.0551025\n",
      "At minibatch 24400, batch loss 0.005852, batch error rate 0.000000%\n",
      "max norm = 2.00607\n",
      "velocity = 0.0385676\n",
      "At minibatch 24500, batch loss 0.008719, batch error rate 0.000000%\n",
      "max norm = 2.00607\n",
      "velocity = 0.0530967\n",
      "After epoch 49: valid_err_rate: 0.015600% currently going ot do 58 epochs\n",
      "At minibatch 24600, batch loss 0.001440, batch error rate 0.000000%\n",
      "max norm = 2.00335\n",
      "velocity = 0.0453223\n",
      "At minibatch 24700, batch loss 0.055860, batch error rate 1.000000%\n",
      "max norm = 2.00588\n",
      "velocity = 0.0481432\n",
      "At minibatch 24800, batch loss 0.029386, batch error rate 1.000000%\n",
      "max norm = 2.00588\n",
      "velocity = 0.028785\n",
      "At minibatch 24900, batch loss 0.033498, batch error rate 3.000000%\n",
      "max norm = 2.00588\n",
      "velocity = 0.0453435\n",
      "At minibatch 25000, batch loss 0.005181, batch error rate 0.000000%\n",
      "max norm = 2.00588\n",
      "velocity = 0.0443059\n",
      "After epoch 50: valid_err_rate: 0.015100% currently going ot do 58 epochs\n",
      "At minibatch 25100, batch loss 0.021127, batch error rate 2.000000%\n",
      "max norm = 2.0013\n",
      "velocity = 0.0343147\n",
      "At minibatch 25200, batch loss 0.103948, batch error rate 2.000000%\n",
      "max norm = 2.00207\n",
      "velocity = 0.0366169\n",
      "At minibatch 25300, batch loss 0.010700, batch error rate 1.000000%\n",
      "max norm = 2.00242\n",
      "velocity = 0.027671\n",
      "At minibatch 25400, batch loss 0.020378, batch error rate 1.000000%\n",
      "max norm = 2.00242\n",
      "velocity = 0.0322752\n",
      "At minibatch 25500, batch loss 0.002501, batch error rate 0.000000%\n",
      "max norm = 2.00242\n",
      "velocity = 0.0398725\n",
      "After epoch 51: valid_err_rate: 0.014700% currently going ot do 67 epochs\n",
      "At minibatch 25600, batch loss 0.013226, batch error rate 0.000000%\n",
      "max norm = 2.00248\n",
      "velocity = 0.0359153\n",
      "At minibatch 25700, batch loss 0.012627, batch error rate 0.000000%\n",
      "max norm = 2.00282\n",
      "velocity = 0.0320825\n",
      "At minibatch 25800, batch loss 0.017987, batch error rate 1.000000%\n",
      "max norm = 2.00282\n",
      "velocity = 0.0352962\n",
      "At minibatch 25900, batch loss 0.020039, batch error rate 1.000000%\n",
      "max norm = 2.00293\n",
      "velocity = 0.0240542\n",
      "At minibatch 26000, batch loss 0.029672, batch error rate 2.000000%\n",
      "max norm = 2.00293\n",
      "velocity = 0.0356732\n",
      "After epoch 52: valid_err_rate: 0.014900% currently going ot do 67 epochs\n",
      "At minibatch 26100, batch loss 0.015131, batch error rate 1.000000%\n",
      "max norm = 2.00254\n",
      "velocity = 0.0280869\n",
      "At minibatch 26200, batch loss 0.004910, batch error rate 0.000000%\n",
      "max norm = 2.00254\n",
      "velocity = 0.0336245\n",
      "At minibatch 26300, batch loss 0.006213, batch error rate 0.000000%\n",
      "max norm = 2.00254\n",
      "velocity = 0.0333952\n",
      "At minibatch 26400, batch loss 0.003967, batch error rate 0.000000%\n",
      "max norm = 2.00254\n",
      "velocity = 0.0283405\n",
      "At minibatch 26500, batch loss 0.057037, batch error rate 1.000000%\n",
      "max norm = 2.00254\n",
      "velocity = 0.0167831\n",
      "After epoch 53: valid_err_rate: 0.015200% currently going ot do 67 epochs\n",
      "At minibatch 26600, batch loss 0.014493, batch error rate 0.000000%\n",
      "max norm = 2.00198\n",
      "velocity = 0.0356021\n",
      "At minibatch 26700, batch loss 0.007687, batch error rate 0.000000%\n",
      "max norm = 2.00236\n",
      "velocity = 0.0268594\n",
      "At minibatch 26800, batch loss 0.030036, batch error rate 2.000000%\n",
      "max norm = 2.00236\n",
      "velocity = 0.0364875\n",
      "At minibatch 26900, batch loss 0.008887, batch error rate 0.000000%\n",
      "max norm = 2.00236\n",
      "velocity = 0.0228482\n",
      "At minibatch 27000, batch loss 0.012528, batch error rate 1.000000%\n",
      "max norm = 2.00241\n",
      "velocity = 0.0331007\n",
      "After epoch 54: valid_err_rate: 0.015000% currently going ot do 67 epochs\n",
      "At minibatch 27100, batch loss 0.029872, batch error rate 2.000000%\n",
      "max norm = 2.00213\n",
      "velocity = 0.023152\n",
      "At minibatch 27200, batch loss 0.012787, batch error rate 0.000000%\n",
      "max norm = 2.0022\n",
      "velocity = 0.0381052\n",
      "At minibatch 27300, batch loss 0.002340, batch error rate 0.000000%\n",
      "max norm = 2.0022\n",
      "velocity = 0.031006\n",
      "At minibatch 27400, batch loss 0.003194, batch error rate 0.000000%\n",
      "max norm = 2.0022\n",
      "velocity = 0.0260639\n",
      "At minibatch 27500, batch loss 0.005535, batch error rate 0.000000%\n",
      "max norm = 2.0022\n",
      "velocity = 0.0319306\n",
      "After epoch 55: valid_err_rate: 0.014600% currently going ot do 72 epochs\n",
      "At minibatch 27600, batch loss 0.003565, batch error rate 0.000000%\n",
      "max norm = 2.0022\n",
      "velocity = 0.0236261\n",
      "At minibatch 27700, batch loss 0.071096, batch error rate 2.000000%\n",
      "max norm = 2.00304\n",
      "velocity = 0.0197323\n",
      "At minibatch 27800, batch loss 0.035773, batch error rate 2.000000%\n",
      "max norm = 2.00304\n",
      "velocity = 0.0371372\n",
      "At minibatch 27900, batch loss 0.002010, batch error rate 0.000000%\n",
      "max norm = 2.00304\n",
      "velocity = 0.0314627\n",
      "At minibatch 28000, batch loss 0.004718, batch error rate 0.000000%\n",
      "max norm = 2.00304\n",
      "velocity = 0.0291937\n",
      "After epoch 56: valid_err_rate: 0.014000% currently going ot do 73 epochs\n",
      "At minibatch 28100, batch loss 0.008539, batch error rate 0.000000%\n",
      "max norm = 2.00146\n",
      "velocity = 0.0372991\n",
      "At minibatch 28200, batch loss 0.014776, batch error rate 0.000000%\n",
      "max norm = 2.00176\n",
      "velocity = 0.029838\n",
      "At minibatch 28300, batch loss 0.006865, batch error rate 0.000000%\n",
      "max norm = 2.00439\n",
      "velocity = 0.0249393\n",
      "At minibatch 28400, batch loss 0.031052, batch error rate 2.000000%\n",
      "max norm = 2.00439\n",
      "velocity = 0.0190949\n",
      "At minibatch 28500, batch loss 0.018295, batch error rate 1.000000%\n",
      "max norm = 2.00439\n",
      "velocity = 0.0181092\n",
      "After epoch 57: valid_err_rate: 0.014400% currently going ot do 73 epochs\n",
      "At minibatch 28600, batch loss 0.002043, batch error rate 0.000000%\n",
      "max norm = 2.00193\n",
      "velocity = 0.0287823\n",
      "At minibatch 28700, batch loss 0.001237, batch error rate 0.000000%\n",
      "max norm = 2.00222\n",
      "velocity = 0.0310462\n",
      "At minibatch 28800, batch loss 0.001945, batch error rate 0.000000%\n",
      "max norm = 2.00222\n",
      "velocity = 0.0303483\n",
      "At minibatch 28900, batch loss 0.012988, batch error rate 0.000000%\n",
      "max norm = 2.00257\n",
      "velocity = 0.0392059\n",
      "At minibatch 29000, batch loss 0.001897, batch error rate 0.000000%\n",
      "max norm = 2.00287\n",
      "velocity = 0.0294714\n",
      "After epoch 58: valid_err_rate: 0.014700% currently going ot do 73 epochs\n",
      "At minibatch 29100, batch loss 0.029079, batch error rate 1.000000%\n",
      "max norm = 2.00171\n",
      "velocity = 0.0311097\n",
      "At minibatch 29200, batch loss 0.005999, batch error rate 0.000000%\n",
      "max norm = 2.00171\n",
      "velocity = 0.0509144\n",
      "At minibatch 29300, batch loss 0.007243, batch error rate 0.000000%\n",
      "max norm = 2.00194\n",
      "velocity = 0.0297131\n",
      "At minibatch 29400, batch loss 0.005660, batch error rate 0.000000%\n",
      "max norm = 2.00256\n",
      "velocity = 0.0162633\n",
      "At minibatch 29500, batch loss 0.033382, batch error rate 1.000000%\n",
      "max norm = 2.00256\n",
      "velocity = 0.0265243\n",
      "After epoch 59: valid_err_rate: 0.014500% currently going ot do 73 epochs\n",
      "At minibatch 29600, batch loss 0.010577, batch error rate 0.000000%\n",
      "max norm = 2.00189\n",
      "velocity = 0.0285532\n",
      "At minibatch 29700, batch loss 0.002737, batch error rate 0.000000%\n",
      "max norm = 2.00215\n",
      "velocity = 0.03806\n",
      "At minibatch 29800, batch loss 0.021097, batch error rate 1.000000%\n",
      "max norm = 2.00215\n",
      "velocity = 0.0244438\n",
      "At minibatch 29900, batch loss 0.026617, batch error rate 2.000000%\n",
      "max norm = 2.00222\n",
      "velocity = 0.0371795\n",
      "At minibatch 30000, batch loss 0.006760, batch error rate 0.000000%\n",
      "max norm = 2.00222\n",
      "velocity = 0.0330638\n",
      "After epoch 60: valid_err_rate: 0.014400% currently going ot do 73 epochs\n",
      "At minibatch 30100, batch loss 0.004451, batch error rate 0.000000%\n",
      "max norm = 2.00193\n",
      "velocity = 0.0312805\n",
      "At minibatch 30200, batch loss 0.001274, batch error rate 0.000000%\n",
      "max norm = 2.00193\n",
      "velocity = 0.0212059\n",
      "At minibatch 30300, batch loss 0.017755, batch error rate 1.000000%\n",
      "max norm = 2.00193\n",
      "velocity = 0.00579168\n",
      "At minibatch 30400, batch loss 0.020615, batch error rate 1.000000%\n",
      "max norm = 2.00193\n",
      "velocity = 0.0316599\n",
      "At minibatch 30500, batch loss 0.003157, batch error rate 0.000000%\n",
      "max norm = 2.00193\n",
      "velocity = 0.0266008\n",
      "After epoch 61: valid_err_rate: 0.013900% currently going ot do 80 epochs\n",
      "At minibatch 30600, batch loss 0.031448, batch error rate 2.000000%\n",
      "max norm = 2.00155\n",
      "velocity = 0.0281947\n",
      "At minibatch 30700, batch loss 0.005395, batch error rate 0.000000%\n",
      "max norm = 2.00155\n",
      "velocity = 0.0145417\n",
      "At minibatch 30800, batch loss 0.003608, batch error rate 0.000000%\n",
      "max norm = 2.00155\n",
      "velocity = 0.0309807\n",
      "At minibatch 30900, batch loss 0.010236, batch error rate 0.000000%\n",
      "max norm = 2.00155\n",
      "velocity = 0.0211157\n",
      "At minibatch 31000, batch loss 0.051126, batch error rate 1.000000%\n",
      "max norm = 2.00155\n",
      "velocity = 0.017278\n",
      "After epoch 62: valid_err_rate: 0.014700% currently going ot do 80 epochs\n",
      "At minibatch 31100, batch loss 0.014291, batch error rate 1.000000%\n",
      "max norm = 2.00141\n",
      "velocity = 0.0201886\n",
      "At minibatch 31200, batch loss 0.011574, batch error rate 1.000000%\n",
      "max norm = 2.00152\n",
      "velocity = 0.0226623\n",
      "At minibatch 31300, batch loss 0.004019, batch error rate 0.000000%\n",
      "max norm = 2.00152\n",
      "velocity = 0.0137061\n",
      "At minibatch 31400, batch loss 0.022988, batch error rate 1.000000%\n",
      "max norm = 2.00152\n",
      "velocity = 0.019655\n",
      "At minibatch 31500, batch loss 0.046860, batch error rate 2.000000%\n",
      "max norm = 2.00152\n",
      "velocity = 0.0244736\n",
      "After epoch 63: valid_err_rate: 0.014900% currently going ot do 80 epochs\n",
      "At minibatch 31600, batch loss 0.027462, batch error rate 2.000000%\n",
      "max norm = 2.00156\n",
      "velocity = 0.0324383\n",
      "At minibatch 31700, batch loss 0.001638, batch error rate 0.000000%\n",
      "max norm = 2.00156\n",
      "velocity = 0.0218521\n",
      "At minibatch 31800, batch loss 0.000499, batch error rate 0.000000%\n",
      "max norm = 2.00193\n",
      "velocity = 0.0193205\n",
      "At minibatch 31900, batch loss 0.005622, batch error rate 0.000000%\n",
      "max norm = 2.00218\n",
      "velocity = 0.0248369\n",
      "At minibatch 32000, batch loss 0.010202, batch error rate 0.000000%\n",
      "max norm = 2.00218\n",
      "velocity = 0.0277967\n",
      "After epoch 64: valid_err_rate: 0.014400% currently going ot do 80 epochs\n",
      "At minibatch 32100, batch loss 0.003159, batch error rate 0.000000%\n",
      "max norm = 2.00177\n",
      "velocity = 0.0187062\n",
      "At minibatch 32200, batch loss 0.011639, batch error rate 0.000000%\n",
      "max norm = 2.00177\n",
      "velocity = 0.0140965\n",
      "Setting network parameters from after epoch 61\n",
      "Test error rate: 0.014500\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAFnCAYAAAAGxCvgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nD0VRFEVRihU1PhRFURRFKVb+H6TSucElN0tUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f415a9161d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "# TODO: pick a network architecture here. The one below is just \n",
    "# softmax regression\n",
    "#\n",
    "\n",
    "net2 = FeedForwardNet([\n",
    "        DropoutLayer(drop_p=0.1),\n",
    "        AffineLayer(784,1500),\n",
    "        ReLULayer(),\n",
    "        DropoutLayer(drop_p=0.3),\n",
    "        AffineLayer(1500, 1000),\n",
    "        TanhLayer(),\n",
    "        DropoutLayer(drop_p=0.5),\n",
    "        AffineLayer(1000, 10),\n",
    "        SoftMaxLayer()\n",
    "        ])\n",
    "SGD(net2, mnist_train_stream, mnist_validation_stream, mnist_test_stream, alpha=-0.2, normlimit=2)\n",
    "\n",
    "print \"Test error rate: %f\" % (compute_error_rate(net2, mnist_test_stream), )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5 [3p bonus]\n",
    "\n",
    "Implement convolutional and max-pooling layers and (without dropout) get a test error rate below 1.5%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 6 [1-3p bonus]\n",
    "\n",
    "Implement a data augmentation method (e.g. rotations, noise, crops) that will yield a significant test error rate reduction for your network. Number of bonus points depends on the ingenuity of your solution and error rate gains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
